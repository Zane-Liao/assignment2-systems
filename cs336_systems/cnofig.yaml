model_args:
  vocab_size: 10000
  context_length: 256
  d_model: 512
  num_layers: 4
  num_heads: 16
  d_ff: 1344
  rope_theta: 10000.0

training_args:
  device: "cuda" # Train device, If you have GPU, Please Change to "cuda" else "cpu"
  batch_size: 64
  num_epochs: 5
  learning_rate: 0.0003
  max_iters: 50000
  eval_interval: 200
  warmup_steps: 200
  lr_decay_steps: 30000
  min_lr: 3.0e-6
  gradient_clip_val: 1.0

data_args:
  train_data_path: "data/train.bin"
  valid_data_path: "data/valid.bin"
  vocab_path: "tokenizer/vocab/vocab_train.json"
  merges_path: "tokenizer/vocab/merges.txt"
  checkpoint_dir: "checkpoints"
  resume_from_checkpoint: False
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Large Language Model from Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load All Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import yaml\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from data import get_batch, load, save\n",
    "from tokenizer import Tokenizer, train_bpe, PAT_GPT2, PAT_SPECIAL_TOKEN\n",
    "from cs336_basics.modules.layers import TransformerLM\n",
    "from cs336_basics.modules.loss import CrossEntropyLoss\n",
    "from cs336_basics.modules.activation import GLU, Softmax\n",
    "from cs336_basics.modules.optimizer import SGD, AdamW, compute_lr, gradient_cliping\n",
    "from tokenizers import Tokenizer as HFTokenizer\n",
    "from init_weights import init_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Configuration ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Configuration ---\")\n",
    "with open('config.yaml', 'r') as f:\n",
    "    load = yaml.safe_load(f)\n",
    "        \n",
    "model_args = load['model_args']\n",
    "training_args = load['training_args']\n",
    "data_args = load['data_args']\n",
    "    \n",
    "os.makedirs(data_args['checkpoint_dir'], exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Initial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initializing ---\n",
      "Using device: cuda\n",
      "Tokenizer loaded. Vocab size: 10000\n",
      "Model created with 17,576,448 parameters.\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Initializing ---\")\n",
    "device = torch.device(training_args['device'] if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "    \n",
    "# tokenizer = Tokenizer.from_files(\n",
    "#     vocab_filepath=data_args['vocab_path'],\n",
    "#     merges_filepath=data_args['merges_path']\n",
    "#     )\n",
    "\n",
    "hf_tokenizer = HFTokenizer.from_file(data_args['vocab_path'])\n",
    "\n",
    "tokenizer = Tokenizer(\n",
    "    vocab={token: idx for token, idx in hf_tokenizer.get_vocab().items()},\n",
    "    merges=data_args['merges_path']\n",
    ")\n",
    "\n",
    "model_args['vocab_size'] = len(tokenizer.vocab)\n",
    "print(f\"Tokenizer loaded. Vocab size: {model_args['vocab_size']}\")\n",
    "\n",
    "model = torch.compile(TransformerLM(**model_args)).to(device)\n",
    "init_weights(model)\n",
    "print(f\"Model created with {model.get_num_params():,} parameters.\")\n",
    "    \n",
    "optimizer = AdamW(model.parameters(), lr=training_args['learning_rate'])\n",
    "    \n",
    "loss_init = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Data with np.memmap ---\n",
      "Train data tokens: 3,038,773,016, Val data tokens: 74,004,384\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Loading Data with np.memmap ---\")\n",
    "train_data = np.memmap(data_args['train_data_path'], dtype=np.uint16, mode='r')\n",
    "valid_data = np.memmap(data_args['valid_data_path'], dtype=np.uint16, mode='r')\n",
    "print(f\"Train data tokens: {len(train_data):,}, Val data tokens: {len(valid_data):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Resume training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_iter = 0\n",
    "if data_args['resume_from_checkpoint']:\n",
    "    print(f\"Resuming training from {data_args['resume_from_checkpoint']}\")\n",
    "    start_iter = load(data_args['resume_from_checkpoint'], model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    eval_iters = 100\n",
    "    for _ in range(eval_iters):\n",
    "        x, y = get_batch(valid_data, training_args['batch_size'], model_args['context_length'], device)\n",
    "        logits = model(x)\n",
    "        loss = loss_init(logits.view(-1, model_args['vocab_size']), y.view(-1))\n",
    "        valid_loss += loss.item()\n",
    "    model.train()\n",
    "    return valid_loss / eval_iters  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Begin Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoints/model_iter_40000.pt iterations: 40000\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "from data import load\n",
    "\n",
    "ckpts = glob.glob(os.path.join(data_args['checkpoint_dir'], \"model_iter_*.pt\"))\n",
    "if ckpts:\n",
    "    latest_ckpt = max(ckpts, key=os.path.getctime)\n",
    "    start_iter = load(latest_ckpt, model, optimizer) + 1\n",
    "else:\n",
    "    print(\"From Scratch Training\")\n",
    "    start_iter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Training Loop ---\n",
      "Iter 40010/50000, Train Loss: 3.7798, LR: 0.000002, GradNorm: 0.4239, Time: 10767.9ms\n",
      "Iter 40020/50000, Train Loss: 3.7747, LR: 0.000002, GradNorm: 0.4210, Time: 1707.3ms\n",
      "Iter 40030/50000, Train Loss: 3.7277, LR: 0.000002, GradNorm: 0.4169, Time: 1705.4ms\n",
      "Iter 40040/50000, Train Loss: 3.7441, LR: 0.000002, GradNorm: 0.4398, Time: 1706.4ms\n",
      "Iter 40050/50000, Train Loss: 3.7717, LR: 0.000002, GradNorm: 0.4089, Time: 1706.0ms\n",
      "Iter 40060/50000, Train Loss: 3.7539, LR: 0.000002, GradNorm: 0.4196, Time: 1708.1ms\n",
      "Iter 40070/50000, Train Loss: 3.7542, LR: 0.000002, GradNorm: 0.4073, Time: 1702.3ms\n",
      "Iter 40080/50000, Train Loss: 3.7773, LR: 0.000002, GradNorm: 0.4091, Time: 1697.9ms\n",
      "Iter 40090/50000, Train Loss: 3.7176, LR: 0.000002, GradNorm: 0.4128, Time: 1699.9ms\n",
      "Iter 40100/50000, Train Loss: 3.7563, LR: 0.000002, GradNorm: 0.4101, Time: 1699.9ms\n",
      "Iter 40110/50000, Train Loss: 3.7274, LR: 0.000002, GradNorm: 0.4044, Time: 1702.5ms\n",
      "Iter 40120/50000, Train Loss: 3.7500, LR: 0.000002, GradNorm: 0.4110, Time: 1701.5ms\n",
      "Iter 40130/50000, Train Loss: 3.7657, LR: 0.000002, GradNorm: 0.4154, Time: 1700.9ms\n",
      "Iter 40140/50000, Train Loss: 3.7858, LR: 0.000002, GradNorm: 0.4158, Time: 1704.2ms\n",
      "Iter 40150/50000, Train Loss: 3.7464, LR: 0.000002, GradNorm: 0.4187, Time: 1702.9ms\n",
      "Iter 40160/50000, Train Loss: 3.7235, LR: 0.000002, GradNorm: 0.4140, Time: 1703.4ms\n",
      "Iter 40170/50000, Train Loss: 3.7525, LR: 0.000002, GradNorm: 0.4075, Time: 1704.8ms\n",
      "Iter 40180/50000, Train Loss: 3.8100, LR: 0.000002, GradNorm: 0.4313, Time: 1704.1ms\n",
      "Iter 40190/50000, Train Loss: 3.7530, LR: 0.000002, GradNorm: 0.4105, Time: 1705.2ms\n",
      "Iter 40200/50000, Train Loss: 3.7285, LR: 0.000002, GradNorm: 0.4306, Time: 1705.9ms\n",
      "Iter 40210/50000, Train Loss: 3.8075, LR: 0.000002, GradNorm: 0.4158, Time: 1704.4ms\n",
      "Iter 40220/50000, Train Loss: 3.7884, LR: 0.000002, GradNorm: 0.4117, Time: 1705.6ms\n",
      "Iter 40230/50000, Train Loss: 3.7859, LR: 0.000002, GradNorm: 0.4210, Time: 1706.7ms\n",
      "Iter 40240/50000, Train Loss: 3.7765, LR: 0.000002, GradNorm: 0.4156, Time: 1705.9ms\n",
      "Iter 40250/50000, Train Loss: 3.7328, LR: 0.000002, GradNorm: 0.4103, Time: 1705.8ms\n",
      "Iter 40260/50000, Train Loss: 3.7593, LR: 0.000002, GradNorm: 0.4185, Time: 1705.6ms\n",
      "Iter 40270/50000, Train Loss: 3.7822, LR: 0.000002, GradNorm: 0.4153, Time: 1706.4ms\n",
      "Iter 40280/50000, Train Loss: 3.7438, LR: 0.000002, GradNorm: 0.4172, Time: 1706.5ms\n",
      "Iter 40290/50000, Train Loss: 3.7887, LR: 0.000002, GradNorm: 0.4271, Time: 1706.4ms\n",
      "Iter 40300/50000, Train Loss: 3.7566, LR: 0.000002, GradNorm: 0.4029, Time: 1706.3ms\n",
      "Iter 40310/50000, Train Loss: 3.7895, LR: 0.000002, GradNorm: 0.4157, Time: 1706.7ms\n",
      "Iter 40320/50000, Train Loss: 3.7947, LR: 0.000002, GradNorm: 0.4084, Time: 1706.7ms\n",
      "Iter 40330/50000, Train Loss: 3.7967, LR: 0.000002, GradNorm: 0.4280, Time: 1708.4ms\n",
      "Iter 40340/50000, Train Loss: 3.7331, LR: 0.000002, GradNorm: 0.4122, Time: 1705.7ms\n",
      "Iter 40350/50000, Train Loss: 3.7275, LR: 0.000002, GradNorm: 0.4053, Time: 1706.4ms\n",
      "Iter 40360/50000, Train Loss: 3.7442, LR: 0.000002, GradNorm: 0.4147, Time: 1706.1ms\n",
      "Iter 40370/50000, Train Loss: 3.8102, LR: 0.000002, GradNorm: 0.4157, Time: 1706.2ms\n",
      "Iter 40380/50000, Train Loss: 3.7743, LR: 0.000002, GradNorm: 0.4190, Time: 1706.3ms\n",
      "Iter 40390/50000, Train Loss: 3.7820, LR: 0.000002, GradNorm: 0.4127, Time: 1706.6ms\n",
      "Iter 40400/50000, Train Loss: 3.7944, LR: 0.000002, GradNorm: 0.4149, Time: 1706.8ms\n",
      "Iter 40410/50000, Train Loss: 3.7451, LR: 0.000002, GradNorm: 0.4096, Time: 1707.7ms\n",
      "Iter 40420/50000, Train Loss: 3.7738, LR: 0.000002, GradNorm: 0.4168, Time: 1705.9ms\n",
      "Iter 40430/50000, Train Loss: 3.7516, LR: 0.000002, GradNorm: 0.4159, Time: 1705.8ms\n",
      "Iter 40440/50000, Train Loss: 3.7797, LR: 0.000002, GradNorm: 0.4195, Time: 1706.5ms\n",
      "Iter 40450/50000, Train Loss: 3.7670, LR: 0.000002, GradNorm: 0.4105, Time: 1707.4ms\n",
      "Iter 40460/50000, Train Loss: 3.7915, LR: 0.000002, GradNorm: 0.4182, Time: 1707.6ms\n",
      "Iter 40470/50000, Train Loss: 3.7996, LR: 0.000002, GradNorm: 0.4132, Time: 1706.6ms\n",
      "Iter 40480/50000, Train Loss: 3.7530, LR: 0.000002, GradNorm: 0.4092, Time: 1706.4ms\n",
      "Iter 40490/50000, Train Loss: 3.7054, LR: 0.000002, GradNorm: 0.4077, Time: 1706.7ms\n",
      "Iter 40500/50000, Train Loss: 3.7131, LR: 0.000002, GradNorm: 0.4086, Time: 1707.3ms\n",
      "Iter 40510/50000, Train Loss: 3.6987, LR: 0.000002, GradNorm: 0.4200, Time: 1705.9ms\n",
      "Iter 40520/50000, Train Loss: 3.7963, LR: 0.000002, GradNorm: 0.4156, Time: 1707.3ms\n",
      "Iter 40530/50000, Train Loss: 3.7490, LR: 0.000002, GradNorm: 0.4161, Time: 1707.4ms\n",
      "Iter 40540/50000, Train Loss: 3.7778, LR: 0.000002, GradNorm: 0.4098, Time: 1706.2ms\n",
      "Iter 40550/50000, Train Loss: 3.8049, LR: 0.000002, GradNorm: 0.4179, Time: 1705.8ms\n",
      "Iter 40560/50000, Train Loss: 3.7415, LR: 0.000002, GradNorm: 0.4151, Time: 1706.9ms\n",
      "Iter 40570/50000, Train Loss: 3.7602, LR: 0.000002, GradNorm: 0.4178, Time: 1707.2ms\n",
      "Iter 40580/50000, Train Loss: 3.7589, LR: 0.000002, GradNorm: 0.4054, Time: 1706.2ms\n",
      "Iter 40590/50000, Train Loss: 3.7691, LR: 0.000002, GradNorm: 0.4033, Time: 1706.3ms\n",
      "Iter 40600/50000, Train Loss: 3.7735, LR: 0.000002, GradNorm: 0.4150, Time: 1706.4ms\n",
      "Iter 40610/50000, Train Loss: 3.7633, LR: 0.000002, GradNorm: 0.4241, Time: 1707.3ms\n",
      "Iter 40620/50000, Train Loss: 3.7259, LR: 0.000002, GradNorm: 0.4096, Time: 1706.0ms\n",
      "Iter 40630/50000, Train Loss: 3.7486, LR: 0.000002, GradNorm: 0.4108, Time: 1706.8ms\n",
      "Iter 40640/50000, Train Loss: 3.7175, LR: 0.000002, GradNorm: 0.4132, Time: 1707.2ms\n",
      "Iter 40650/50000, Train Loss: 3.7634, LR: 0.000002, GradNorm: 0.4101, Time: 1706.1ms\n",
      "Iter 40660/50000, Train Loss: 3.7678, LR: 0.000002, GradNorm: 0.4115, Time: 1706.8ms\n",
      "Iter 40670/50000, Train Loss: 3.7551, LR: 0.000002, GradNorm: 0.4201, Time: 1708.0ms\n",
      "Iter 40680/50000, Train Loss: 3.7869, LR: 0.000002, GradNorm: 0.4161, Time: 1706.4ms\n",
      "Iter 40690/50000, Train Loss: 3.8076, LR: 0.000002, GradNorm: 0.4090, Time: 1705.8ms\n",
      "Iter 40700/50000, Train Loss: 3.7743, LR: 0.000002, GradNorm: 0.4056, Time: 1706.4ms\n",
      "Iter 40710/50000, Train Loss: 3.7768, LR: 0.000002, GradNorm: 0.4139, Time: 1706.2ms\n",
      "Iter 40720/50000, Train Loss: 3.6809, LR: 0.000002, GradNorm: 0.4034, Time: 1706.2ms\n",
      "Iter 40730/50000, Train Loss: 3.7739, LR: 0.000002, GradNorm: 0.4129, Time: 1706.8ms\n",
      "Iter 40740/50000, Train Loss: 3.7713, LR: 0.000002, GradNorm: 0.4138, Time: 1706.7ms\n",
      "Iter 40750/50000, Train Loss: 3.8063, LR: 0.000002, GradNorm: 0.4137, Time: 1706.0ms\n",
      "Iter 40760/50000, Train Loss: 3.7670, LR: 0.000002, GradNorm: 0.4172, Time: 1706.3ms\n",
      "Iter 40770/50000, Train Loss: 3.7545, LR: 0.000002, GradNorm: 0.4140, Time: 1706.8ms\n",
      "Iter 40780/50000, Train Loss: 3.6935, LR: 0.000002, GradNorm: 0.4162, Time: 1708.4ms\n",
      "Iter 40790/50000, Train Loss: 3.7143, LR: 0.000002, GradNorm: 0.4089, Time: 1708.5ms\n",
      "Iter 40800/50000, Train Loss: 3.8161, LR: 0.000002, GradNorm: 0.4216, Time: 1706.5ms\n",
      "Iter 40810/50000, Train Loss: 3.7478, LR: 0.000002, GradNorm: 0.4114, Time: 1707.6ms\n",
      "Iter 40820/50000, Train Loss: 3.7546, LR: 0.000002, GradNorm: 0.4023, Time: 1706.8ms\n",
      "Iter 40830/50000, Train Loss: 3.7268, LR: 0.000002, GradNorm: 0.4099, Time: 1706.4ms\n",
      "Iter 40840/50000, Train Loss: 3.7847, LR: 0.000002, GradNorm: 0.4101, Time: 1706.7ms\n",
      "Iter 40850/50000, Train Loss: 3.7514, LR: 0.000002, GradNorm: 0.4031, Time: 1707.4ms\n",
      "Iter 40860/50000, Train Loss: 3.7472, LR: 0.000002, GradNorm: 0.4120, Time: 1707.8ms\n",
      "Iter 40870/50000, Train Loss: 3.7687, LR: 0.000002, GradNorm: 0.4091, Time: 1706.6ms\n",
      "Iter 40880/50000, Train Loss: 3.7513, LR: 0.000002, GradNorm: 0.4032, Time: 1706.2ms\n",
      "Iter 40890/50000, Train Loss: 3.7836, LR: 0.000002, GradNorm: 0.4149, Time: 1706.2ms\n",
      "Iter 40900/50000, Train Loss: 3.7341, LR: 0.000002, GradNorm: 0.4102, Time: 1706.7ms\n",
      "Iter 40910/50000, Train Loss: 3.8070, LR: 0.000002, GradNorm: 0.4074, Time: 1706.3ms\n",
      "Iter 40920/50000, Train Loss: 3.7617, LR: 0.000002, GradNorm: 0.4080, Time: 1707.6ms\n",
      "Iter 40930/50000, Train Loss: 3.7590, LR: 0.000002, GradNorm: 0.4112, Time: 1706.7ms\n",
      "Iter 40940/50000, Train Loss: 3.7684, LR: 0.000002, GradNorm: 0.4056, Time: 1706.5ms\n",
      "Iter 40950/50000, Train Loss: 3.8078, LR: 0.000002, GradNorm: 0.4154, Time: 1706.2ms\n",
      "Iter 40960/50000, Train Loss: 3.7905, LR: 0.000002, GradNorm: 0.4134, Time: 1707.0ms\n",
      "Iter 40970/50000, Train Loss: 3.7061, LR: 0.000002, GradNorm: 0.4083, Time: 1705.7ms\n",
      "Iter 40980/50000, Train Loss: 3.7240, LR: 0.000002, GradNorm: 0.4062, Time: 1706.2ms\n",
      "Iter 40990/50000, Train Loss: 3.7401, LR: 0.000002, GradNorm: 0.4163, Time: 1708.1ms\n",
      "Iter 41000/50000, Train Loss: 3.7778, LR: 0.000002, GradNorm: 0.4105, Time: 1706.4ms\n",
      "Iter 41010/50000, Train Loss: 3.7287, LR: 0.000002, GradNorm: 0.4090, Time: 1705.7ms\n",
      "Iter 41020/50000, Train Loss: 3.7503, LR: 0.000002, GradNorm: 0.4047, Time: 1706.8ms\n",
      "Iter 41030/50000, Train Loss: 3.7803, LR: 0.000002, GradNorm: 0.4165, Time: 1706.4ms\n",
      "Iter 41040/50000, Train Loss: 3.6918, LR: 0.000002, GradNorm: 0.4025, Time: 1707.9ms\n",
      "Iter 41050/50000, Train Loss: 3.7405, LR: 0.000002, GradNorm: 0.4051, Time: 1706.6ms\n",
      "Iter 41060/50000, Train Loss: 3.8122, LR: 0.000002, GradNorm: 0.4077, Time: 1705.8ms\n",
      "Iter 41070/50000, Train Loss: 3.7574, LR: 0.000002, GradNorm: 0.4067, Time: 1707.1ms\n",
      "Iter 41080/50000, Train Loss: 3.7789, LR: 0.000002, GradNorm: 0.4170, Time: 1706.5ms\n",
      "Iter 41090/50000, Train Loss: 3.7338, LR: 0.000002, GradNorm: 0.4144, Time: 1707.4ms\n",
      "Iter 41100/50000, Train Loss: 3.7452, LR: 0.000002, GradNorm: 0.4183, Time: 1706.1ms\n",
      "Iter 41110/50000, Train Loss: 3.7811, LR: 0.000002, GradNorm: 0.4135, Time: 1706.2ms\n",
      "Iter 41120/50000, Train Loss: 3.7141, LR: 0.000002, GradNorm: 0.4010, Time: 1706.1ms\n",
      "Iter 41130/50000, Train Loss: 3.7745, LR: 0.000002, GradNorm: 0.4121, Time: 1706.1ms\n",
      "Iter 41140/50000, Train Loss: 3.7558, LR: 0.000002, GradNorm: 0.4121, Time: 1707.7ms\n",
      "Iter 41150/50000, Train Loss: 3.7641, LR: 0.000002, GradNorm: 0.4098, Time: 1705.8ms\n",
      "Iter 41160/50000, Train Loss: 3.7670, LR: 0.000002, GradNorm: 0.4104, Time: 1706.2ms\n",
      "Iter 41170/50000, Train Loss: 3.7251, LR: 0.000002, GradNorm: 0.4162, Time: 1706.6ms\n",
      "Iter 41180/50000, Train Loss: 3.7806, LR: 0.000002, GradNorm: 0.4100, Time: 1706.2ms\n",
      "Iter 41190/50000, Train Loss: 3.7174, LR: 0.000002, GradNorm: 0.4072, Time: 1705.7ms\n",
      "Iter 41200/50000, Train Loss: 3.7714, LR: 0.000002, GradNorm: 0.4108, Time: 1706.7ms\n",
      "Iter 41210/50000, Train Loss: 3.7493, LR: 0.000002, GradNorm: 0.4098, Time: 1706.8ms\n",
      "Iter 41220/50000, Train Loss: 3.7837, LR: 0.000002, GradNorm: 0.4104, Time: 1706.4ms\n",
      "Iter 41230/50000, Train Loss: 3.7707, LR: 0.000002, GradNorm: 0.4227, Time: 1706.4ms\n",
      "Iter 41240/50000, Train Loss: 3.8084, LR: 0.000002, GradNorm: 0.4124, Time: 1707.5ms\n",
      "Iter 41250/50000, Train Loss: 3.8016, LR: 0.000002, GradNorm: 0.4096, Time: 1708.4ms\n",
      "Iter 41260/50000, Train Loss: 3.7795, LR: 0.000002, GradNorm: 0.4195, Time: 1705.7ms\n",
      "Iter 41270/50000, Train Loss: 3.7250, LR: 0.000002, GradNorm: 0.4078, Time: 1706.6ms\n",
      "Iter 41280/50000, Train Loss: 3.7798, LR: 0.000002, GradNorm: 0.4117, Time: 1707.1ms\n",
      "Iter 41290/50000, Train Loss: 3.8052, LR: 0.000002, GradNorm: 0.4017, Time: 1706.3ms\n",
      "Iter 41300/50000, Train Loss: 3.7565, LR: 0.000002, GradNorm: 0.4200, Time: 1707.0ms\n",
      "Iter 41310/50000, Train Loss: 3.7662, LR: 0.000002, GradNorm: 0.4127, Time: 1707.3ms\n",
      "Iter 41320/50000, Train Loss: 3.7945, LR: 0.000002, GradNorm: 0.4117, Time: 1706.1ms\n",
      "Iter 41330/50000, Train Loss: 3.7834, LR: 0.000002, GradNorm: 0.4124, Time: 1705.9ms\n",
      "Iter 41340/50000, Train Loss: 3.7561, LR: 0.000002, GradNorm: 0.4230, Time: 1706.6ms\n",
      "Iter 41350/50000, Train Loss: 3.7829, LR: 0.000002, GradNorm: 0.4161, Time: 1707.2ms\n",
      "Iter 41360/50000, Train Loss: 3.7321, LR: 0.000002, GradNorm: 0.4196, Time: 1708.3ms\n",
      "Iter 41370/50000, Train Loss: 3.7478, LR: 0.000002, GradNorm: 0.4089, Time: 1706.1ms\n",
      "Iter 41380/50000, Train Loss: 3.7408, LR: 0.000002, GradNorm: 0.4145, Time: 1706.2ms\n",
      "Iter 41390/50000, Train Loss: 3.7910, LR: 0.000002, GradNorm: 0.4175, Time: 1706.1ms\n",
      "Iter 41400/50000, Train Loss: 3.8176, LR: 0.000002, GradNorm: 0.4110, Time: 1706.4ms\n",
      "Iter 41410/50000, Train Loss: 3.7967, LR: 0.000002, GradNorm: 0.4182, Time: 1706.3ms\n",
      "Iter 41420/50000, Train Loss: 3.6978, LR: 0.000002, GradNorm: 0.4144, Time: 1707.4ms\n",
      "Iter 41430/50000, Train Loss: 3.7189, LR: 0.000002, GradNorm: 0.4086, Time: 1706.3ms\n",
      "Iter 41440/50000, Train Loss: 3.7634, LR: 0.000002, GradNorm: 0.4123, Time: 1706.7ms\n",
      "Iter 41450/50000, Train Loss: 3.7704, LR: 0.000002, GradNorm: 0.4184, Time: 1707.4ms\n",
      "Iter 41460/50000, Train Loss: 3.7821, LR: 0.000002, GradNorm: 0.4132, Time: 1706.6ms\n",
      "Iter 41470/50000, Train Loss: 3.7809, LR: 0.000002, GradNorm: 0.4176, Time: 1708.4ms\n",
      "Iter 41480/50000, Train Loss: 3.7548, LR: 0.000002, GradNorm: 0.4150, Time: 1706.6ms\n",
      "Iter 41490/50000, Train Loss: 3.7118, LR: 0.000002, GradNorm: 0.4123, Time: 1706.4ms\n",
      "Iter 41500/50000, Train Loss: 3.7405, LR: 0.000002, GradNorm: 0.4268, Time: 1707.4ms\n",
      "Iter 41510/50000, Train Loss: 3.7770, LR: 0.000002, GradNorm: 0.4191, Time: 1706.5ms\n",
      "Iter 41520/50000, Train Loss: 3.7859, LR: 0.000002, GradNorm: 0.4116, Time: 1706.6ms\n",
      "Iter 41530/50000, Train Loss: 3.7662, LR: 0.000002, GradNorm: 0.4185, Time: 1706.4ms\n",
      "Iter 41540/50000, Train Loss: 3.7183, LR: 0.000002, GradNorm: 0.4151, Time: 1707.8ms\n",
      "Iter 41550/50000, Train Loss: 3.7339, LR: 0.000002, GradNorm: 0.4103, Time: 1706.0ms\n",
      "Iter 41560/50000, Train Loss: 3.7768, LR: 0.000002, GradNorm: 0.4143, Time: 1707.0ms\n",
      "Iter 41570/50000, Train Loss: 3.7493, LR: 0.000002, GradNorm: 0.4166, Time: 1706.7ms\n",
      "Iter 41580/50000, Train Loss: 3.7285, LR: 0.000002, GradNorm: 0.4059, Time: 1706.7ms\n",
      "Iter 41590/50000, Train Loss: 3.7340, LR: 0.000002, GradNorm: 0.4174, Time: 1706.4ms\n",
      "Iter 41600/50000, Train Loss: 3.7968, LR: 0.000002, GradNorm: 0.4102, Time: 1706.3ms\n",
      "Iter 41610/50000, Train Loss: 3.7886, LR: 0.000002, GradNorm: 0.4065, Time: 1707.6ms\n",
      "Iter 41620/50000, Train Loss: 3.7092, LR: 0.000002, GradNorm: 0.4083, Time: 1706.5ms\n",
      "Iter 41630/50000, Train Loss: 3.7637, LR: 0.000002, GradNorm: 0.4296, Time: 1706.4ms\n",
      "Iter 41640/50000, Train Loss: 3.7307, LR: 0.000002, GradNorm: 0.4085, Time: 1707.7ms\n",
      "Iter 41650/50000, Train Loss: 3.6868, LR: 0.000002, GradNorm: 0.4099, Time: 1707.0ms\n",
      "Iter 41660/50000, Train Loss: 3.7164, LR: 0.000002, GradNorm: 0.4064, Time: 1708.4ms\n",
      "Iter 41670/50000, Train Loss: 3.7627, LR: 0.000002, GradNorm: 0.4070, Time: 1707.3ms\n",
      "Iter 41680/50000, Train Loss: 3.7084, LR: 0.000002, GradNorm: 0.4109, Time: 1706.6ms\n",
      "Iter 41690/50000, Train Loss: 3.7888, LR: 0.000002, GradNorm: 0.4171, Time: 1707.5ms\n",
      "Iter 41700/50000, Train Loss: 3.7593, LR: 0.000002, GradNorm: 0.4046, Time: 1708.0ms\n",
      "Iter 41710/50000, Train Loss: 3.7495, LR: 0.000002, GradNorm: 0.4239, Time: 1707.0ms\n",
      "Iter 41720/50000, Train Loss: 3.7121, LR: 0.000002, GradNorm: 0.4266, Time: 1705.7ms\n",
      "Iter 41730/50000, Train Loss: 3.7888, LR: 0.000002, GradNorm: 0.4178, Time: 1707.4ms\n",
      "Iter 41740/50000, Train Loss: 3.7410, LR: 0.000002, GradNorm: 0.4147, Time: 1708.1ms\n",
      "Iter 41750/50000, Train Loss: 3.7721, LR: 0.000002, GradNorm: 0.4159, Time: 1706.5ms\n",
      "Iter 41760/50000, Train Loss: 3.7365, LR: 0.000002, GradNorm: 0.4122, Time: 1706.8ms\n",
      "Iter 41770/50000, Train Loss: 3.7568, LR: 0.000002, GradNorm: 0.4016, Time: 1707.6ms\n",
      "Iter 41780/50000, Train Loss: 3.7637, LR: 0.000002, GradNorm: 0.4201, Time: 1707.2ms\n",
      "Iter 41790/50000, Train Loss: 3.7323, LR: 0.000002, GradNorm: 0.4147, Time: 1706.6ms\n",
      "Iter 41800/50000, Train Loss: 3.7511, LR: 0.000002, GradNorm: 0.4180, Time: 1706.1ms\n",
      "Iter 41810/50000, Train Loss: 3.8068, LR: 0.000002, GradNorm: 0.4128, Time: 1705.9ms\n",
      "Iter 41820/50000, Train Loss: 3.8210, LR: 0.000002, GradNorm: 0.4209, Time: 1707.3ms\n",
      "Iter 41830/50000, Train Loss: 3.7375, LR: 0.000002, GradNorm: 0.4049, Time: 1708.4ms\n",
      "Iter 41840/50000, Train Loss: 3.7013, LR: 0.000002, GradNorm: 0.4099, Time: 1709.0ms\n",
      "Iter 41850/50000, Train Loss: 3.7456, LR: 0.000002, GradNorm: 0.4128, Time: 1707.2ms\n",
      "Iter 41860/50000, Train Loss: 3.7471, LR: 0.000002, GradNorm: 0.4103, Time: 1707.0ms\n",
      "Iter 41870/50000, Train Loss: 3.6584, LR: 0.000002, GradNorm: 0.4109, Time: 1707.7ms\n",
      "Iter 41880/50000, Train Loss: 3.7000, LR: 0.000002, GradNorm: 0.4118, Time: 1707.8ms\n",
      "Iter 41890/50000, Train Loss: 3.7724, LR: 0.000002, GradNorm: 0.4205, Time: 1707.1ms\n",
      "Iter 41900/50000, Train Loss: 3.7125, LR: 0.000002, GradNorm: 0.4184, Time: 1707.5ms\n",
      "Iter 41910/50000, Train Loss: 3.6961, LR: 0.000002, GradNorm: 0.4095, Time: 1707.2ms\n",
      "Iter 41920/50000, Train Loss: 3.7739, LR: 0.000002, GradNorm: 0.4124, Time: 1707.4ms\n",
      "Iter 41930/50000, Train Loss: 3.8083, LR: 0.000002, GradNorm: 0.4153, Time: 1708.6ms\n",
      "Iter 41940/50000, Train Loss: 3.6864, LR: 0.000002, GradNorm: 0.4149, Time: 1708.7ms\n",
      "Iter 41950/50000, Train Loss: 3.7347, LR: 0.000002, GradNorm: 0.4211, Time: 1706.1ms\n",
      "Iter 41960/50000, Train Loss: 3.7429, LR: 0.000002, GradNorm: 0.4085, Time: 1707.9ms\n",
      "Iter 41970/50000, Train Loss: 3.7401, LR: 0.000002, GradNorm: 0.4135, Time: 1707.6ms\n",
      "Iter 41980/50000, Train Loss: 3.7540, LR: 0.000002, GradNorm: 0.4144, Time: 1707.3ms\n",
      "Iter 41990/50000, Train Loss: 3.7387, LR: 0.000002, GradNorm: 0.4159, Time: 1706.2ms\n",
      "Iter 42000/50000, Train Loss: 3.7406, LR: 0.000002, GradNorm: 0.4165, Time: 1707.2ms\n",
      "Iter 42010/50000, Train Loss: 3.7451, LR: 0.000002, GradNorm: 0.4035, Time: 1707.3ms\n",
      "Iter 42020/50000, Train Loss: 3.7388, LR: 0.000002, GradNorm: 0.4106, Time: 1708.0ms\n",
      "Iter 42030/50000, Train Loss: 3.7002, LR: 0.000002, GradNorm: 0.4097, Time: 1707.7ms\n",
      "Iter 42040/50000, Train Loss: 3.7714, LR: 0.000002, GradNorm: 0.4237, Time: 1708.7ms\n",
      "Iter 42050/50000, Train Loss: 3.7871, LR: 0.000002, GradNorm: 0.4052, Time: 1707.0ms\n",
      "Iter 42060/50000, Train Loss: 3.6917, LR: 0.000002, GradNorm: 0.4459, Time: 1706.8ms\n",
      "Iter 42070/50000, Train Loss: 3.7277, LR: 0.000002, GradNorm: 0.4146, Time: 1707.3ms\n",
      "Iter 42080/50000, Train Loss: 3.7567, LR: 0.000002, GradNorm: 0.4078, Time: 1708.3ms\n",
      "Iter 42090/50000, Train Loss: 3.7615, LR: 0.000002, GradNorm: 0.4201, Time: 1706.4ms\n",
      "Iter 42100/50000, Train Loss: 3.7306, LR: 0.000002, GradNorm: 0.4115, Time: 1707.0ms\n",
      "Iter 42110/50000, Train Loss: 3.7580, LR: 0.000002, GradNorm: 0.4113, Time: 1706.5ms\n",
      "Iter 42120/50000, Train Loss: 3.6961, LR: 0.000002, GradNorm: 0.4269, Time: 1708.5ms\n",
      "Iter 42130/50000, Train Loss: 3.7649, LR: 0.000002, GradNorm: 0.4129, Time: 1707.5ms\n",
      "Iter 42140/50000, Train Loss: 3.6819, LR: 0.000002, GradNorm: 0.4146, Time: 1707.4ms\n",
      "Iter 42150/50000, Train Loss: 3.8163, LR: 0.000002, GradNorm: 0.4125, Time: 1707.2ms\n",
      "Iter 42160/50000, Train Loss: 3.8179, LR: 0.000002, GradNorm: 0.4101, Time: 1706.9ms\n",
      "Iter 42170/50000, Train Loss: 3.7301, LR: 0.000002, GradNorm: 0.4127, Time: 1707.1ms\n",
      "Iter 42180/50000, Train Loss: 3.7056, LR: 0.000002, GradNorm: 0.4110, Time: 1707.5ms\n",
      "Iter 42190/50000, Train Loss: 3.7377, LR: 0.000002, GradNorm: 0.4144, Time: 1706.5ms\n",
      "Iter 42200/50000, Train Loss: 3.8167, LR: 0.000002, GradNorm: 0.4134, Time: 1705.9ms\n",
      "Iter 42210/50000, Train Loss: 3.7598, LR: 0.000002, GradNorm: 0.4124, Time: 1706.8ms\n",
      "Iter 42220/50000, Train Loss: 3.7709, LR: 0.000002, GradNorm: 0.4058, Time: 1708.7ms\n",
      "Iter 42230/50000, Train Loss: 3.7852, LR: 0.000002, GradNorm: 0.4137, Time: 1706.7ms\n",
      "Iter 42240/50000, Train Loss: 3.7787, LR: 0.000002, GradNorm: 0.4787, Time: 1707.0ms\n",
      "Iter 42250/50000, Train Loss: 3.7721, LR: 0.000002, GradNorm: 0.4154, Time: 1707.2ms\n",
      "Iter 42260/50000, Train Loss: 3.7091, LR: 0.000002, GradNorm: 0.4257, Time: 1706.6ms\n",
      "Iter 42270/50000, Train Loss: 3.7761, LR: 0.000002, GradNorm: 0.4138, Time: 1707.4ms\n",
      "Iter 42280/50000, Train Loss: 3.7016, LR: 0.000002, GradNorm: 0.4100, Time: 1707.5ms\n",
      "Iter 42290/50000, Train Loss: 3.7000, LR: 0.000002, GradNorm: 0.4132, Time: 1706.8ms\n",
      "Iter 42300/50000, Train Loss: 3.7495, LR: 0.000002, GradNorm: 0.4147, Time: 1707.3ms\n",
      "Iter 42310/50000, Train Loss: 3.7992, LR: 0.000002, GradNorm: 0.4079, Time: 1707.8ms\n",
      "Iter 42320/50000, Train Loss: 3.7989, LR: 0.000002, GradNorm: 0.4045, Time: 1708.8ms\n",
      "Iter 42330/50000, Train Loss: 3.6875, LR: 0.000002, GradNorm: 0.4089, Time: 1707.6ms\n",
      "Iter 42340/50000, Train Loss: 3.7785, LR: 0.000002, GradNorm: 0.4094, Time: 1706.7ms\n",
      "Iter 42350/50000, Train Loss: 3.7541, LR: 0.000002, GradNorm: 0.4209, Time: 1706.7ms\n",
      "Iter 42360/50000, Train Loss: 3.7866, LR: 0.000002, GradNorm: 0.4090, Time: 1708.0ms\n",
      "Iter 42370/50000, Train Loss: 3.7043, LR: 0.000002, GradNorm: 0.4182, Time: 1707.1ms\n",
      "Iter 42380/50000, Train Loss: 3.7328, LR: 0.000002, GradNorm: 0.4059, Time: 1707.5ms\n",
      "Iter 42390/50000, Train Loss: 3.7968, LR: 0.000002, GradNorm: 0.4246, Time: 1706.6ms\n",
      "Iter 42400/50000, Train Loss: 3.7383, LR: 0.000002, GradNorm: 0.4103, Time: 1707.8ms\n",
      "Iter 42410/50000, Train Loss: 3.7245, LR: 0.000002, GradNorm: 0.4095, Time: 1707.1ms\n",
      "Iter 42420/50000, Train Loss: 3.7498, LR: 0.000002, GradNorm: 0.4005, Time: 1708.0ms\n",
      "Iter 42430/50000, Train Loss: 3.7167, LR: 0.000002, GradNorm: 0.4044, Time: 1707.6ms\n",
      "Iter 42440/50000, Train Loss: 3.7655, LR: 0.000002, GradNorm: 0.4115, Time: 1707.6ms\n",
      "Iter 42450/50000, Train Loss: 3.7050, LR: 0.000002, GradNorm: 0.4161, Time: 1706.9ms\n",
      "Iter 42460/50000, Train Loss: 3.7439, LR: 0.000002, GradNorm: 0.4156, Time: 1707.5ms\n",
      "Iter 42470/50000, Train Loss: 3.7503, LR: 0.000002, GradNorm: 0.4099, Time: 1707.5ms\n",
      "Iter 42480/50000, Train Loss: 3.8377, LR: 0.000002, GradNorm: 0.4232, Time: 1707.6ms\n",
      "Iter 42490/50000, Train Loss: 3.7330, LR: 0.000002, GradNorm: 0.4089, Time: 1706.9ms\n",
      "Iter 42500/50000, Train Loss: 3.7397, LR: 0.000002, GradNorm: 0.4247, Time: 1707.1ms\n",
      "Iter 42510/50000, Train Loss: 3.6859, LR: 0.000002, GradNorm: 0.4068, Time: 1708.8ms\n",
      "Iter 42520/50000, Train Loss: 3.7731, LR: 0.000002, GradNorm: 0.4227, Time: 1707.5ms\n",
      "Iter 42530/50000, Train Loss: 3.7954, LR: 0.000002, GradNorm: 0.4204, Time: 1706.7ms\n",
      "Iter 42540/50000, Train Loss: 3.7326, LR: 0.000002, GradNorm: 0.4249, Time: 1705.7ms\n",
      "Iter 42550/50000, Train Loss: 3.7782, LR: 0.000002, GradNorm: 0.4134, Time: 1707.8ms\n",
      "Iter 42560/50000, Train Loss: 3.7254, LR: 0.000002, GradNorm: 0.4169, Time: 1706.9ms\n",
      "Iter 42570/50000, Train Loss: 3.7192, LR: 0.000002, GradNorm: 0.4071, Time: 1707.2ms\n",
      "Iter 42580/50000, Train Loss: 3.7933, LR: 0.000002, GradNorm: 0.4162, Time: 1706.5ms\n",
      "Iter 42590/50000, Train Loss: 3.7602, LR: 0.000002, GradNorm: 0.4261, Time: 1707.4ms\n",
      "Iter 42600/50000, Train Loss: 3.7892, LR: 0.000002, GradNorm: 0.4239, Time: 1707.2ms\n",
      "Iter 42610/50000, Train Loss: 3.7406, LR: 0.000002, GradNorm: 0.4092, Time: 1708.2ms\n",
      "Iter 42620/50000, Train Loss: 3.8128, LR: 0.000002, GradNorm: 0.4208, Time: 1707.2ms\n",
      "Iter 42630/50000, Train Loss: 3.7631, LR: 0.000002, GradNorm: 0.4033, Time: 1706.8ms\n",
      "Iter 42640/50000, Train Loss: 3.7440, LR: 0.000002, GradNorm: 0.4056, Time: 1706.7ms\n",
      "Iter 42650/50000, Train Loss: 3.7458, LR: 0.000002, GradNorm: 0.4205, Time: 1706.7ms\n",
      "Iter 42660/50000, Train Loss: 3.7516, LR: 0.000002, GradNorm: 0.4146, Time: 1708.2ms\n",
      "Iter 42670/50000, Train Loss: 3.7743, LR: 0.000002, GradNorm: 0.4067, Time: 1707.1ms\n",
      "Iter 42680/50000, Train Loss: 3.7586, LR: 0.000002, GradNorm: 0.4163, Time: 1706.6ms\n",
      "Iter 42690/50000, Train Loss: 3.7191, LR: 0.000002, GradNorm: 0.4278, Time: 1707.9ms\n",
      "Iter 42700/50000, Train Loss: 3.7325, LR: 0.000002, GradNorm: 0.4192, Time: 1707.7ms\n",
      "Iter 42710/50000, Train Loss: 3.7271, LR: 0.000002, GradNorm: 0.4181, Time: 1707.3ms\n",
      "Iter 42720/50000, Train Loss: 3.7639, LR: 0.000002, GradNorm: 0.4172, Time: 1705.9ms\n",
      "Iter 42730/50000, Train Loss: 3.7907, LR: 0.000002, GradNorm: 0.4124, Time: 1707.3ms\n",
      "Iter 42740/50000, Train Loss: 3.7593, LR: 0.000002, GradNorm: 0.4111, Time: 1708.5ms\n",
      "Iter 42750/50000, Train Loss: 3.7359, LR: 0.000002, GradNorm: 0.4119, Time: 1706.6ms\n",
      "Iter 42760/50000, Train Loss: 3.7338, LR: 0.000002, GradNorm: 0.4070, Time: 1707.7ms\n",
      "Iter 42770/50000, Train Loss: 3.7652, LR: 0.000002, GradNorm: 0.4270, Time: 1706.4ms\n",
      "Iter 42780/50000, Train Loss: 3.7304, LR: 0.000002, GradNorm: 0.4177, Time: 1708.2ms\n",
      "Iter 42790/50000, Train Loss: 3.7486, LR: 0.000002, GradNorm: 0.4095, Time: 1707.3ms\n",
      "Iter 42800/50000, Train Loss: 3.6956, LR: 0.000002, GradNorm: 0.4179, Time: 1708.2ms\n",
      "Iter 42810/50000, Train Loss: 3.7237, LR: 0.000002, GradNorm: 0.4137, Time: 1707.1ms\n",
      "Iter 42820/50000, Train Loss: 3.7351, LR: 0.000002, GradNorm: 0.4104, Time: 1707.0ms\n",
      "Iter 42830/50000, Train Loss: 3.7326, LR: 0.000002, GradNorm: 0.4109, Time: 1707.4ms\n",
      "Iter 42840/50000, Train Loss: 3.6904, LR: 0.000002, GradNorm: 0.4167, Time: 1707.9ms\n",
      "Iter 42850/50000, Train Loss: 3.7563, LR: 0.000002, GradNorm: 0.4186, Time: 1706.5ms\n",
      "Iter 42860/50000, Train Loss: 3.7712, LR: 0.000002, GradNorm: 0.4120, Time: 1706.9ms\n",
      "Iter 42870/50000, Train Loss: 3.7205, LR: 0.000002, GradNorm: 0.4164, Time: 1705.9ms\n",
      "Iter 42880/50000, Train Loss: 3.6996, LR: 0.000002, GradNorm: 0.4180, Time: 1708.3ms\n",
      "Iter 42890/50000, Train Loss: 3.7382, LR: 0.000002, GradNorm: 0.4157, Time: 1707.1ms\n",
      "Iter 42900/50000, Train Loss: 3.7529, LR: 0.000002, GradNorm: 0.4181, Time: 1706.1ms\n",
      "Iter 42910/50000, Train Loss: 3.7343, LR: 0.000002, GradNorm: 0.4179, Time: 1706.8ms\n",
      "Iter 42920/50000, Train Loss: 3.7090, LR: 0.000002, GradNorm: 0.4157, Time: 1706.9ms\n",
      "Iter 42930/50000, Train Loss: 3.7543, LR: 0.000002, GradNorm: 0.4209, Time: 1707.2ms\n",
      "Iter 42940/50000, Train Loss: 3.7263, LR: 0.000002, GradNorm: 0.4098, Time: 1705.9ms\n",
      "Iter 42950/50000, Train Loss: 3.7641, LR: 0.000002, GradNorm: 0.4153, Time: 1706.2ms\n",
      "Iter 42960/50000, Train Loss: 3.7779, LR: 0.000002, GradNorm: 0.4198, Time: 1707.2ms\n",
      "Iter 42970/50000, Train Loss: 3.7046, LR: 0.000002, GradNorm: 0.4151, Time: 1708.0ms\n",
      "Iter 42980/50000, Train Loss: 3.7783, LR: 0.000002, GradNorm: 0.4128, Time: 1706.9ms\n",
      "Iter 42990/50000, Train Loss: 3.7857, LR: 0.000002, GradNorm: 0.4189, Time: 1707.1ms\n",
      "Iter 43000/50000, Train Loss: 3.7967, LR: 0.000002, GradNorm: 0.4152, Time: 1707.6ms\n",
      "Iter 43010/50000, Train Loss: 3.7718, LR: 0.000002, GradNorm: 0.4198, Time: 1707.5ms\n",
      "Iter 43020/50000, Train Loss: 3.7164, LR: 0.000002, GradNorm: 0.4315, Time: 1705.5ms\n",
      "Iter 43030/50000, Train Loss: 3.7303, LR: 0.000002, GradNorm: 0.4116, Time: 1706.0ms\n",
      "Iter 43040/50000, Train Loss: 3.7469, LR: 0.000002, GradNorm: 0.4115, Time: 1707.7ms\n",
      "Iter 43050/50000, Train Loss: 3.6978, LR: 0.000002, GradNorm: 0.4041, Time: 1707.5ms\n",
      "Iter 43060/50000, Train Loss: 3.7462, LR: 0.000002, GradNorm: 0.4150, Time: 1706.2ms\n",
      "Iter 43070/50000, Train Loss: 3.7159, LR: 0.000002, GradNorm: 0.4103, Time: 1707.8ms\n",
      "Iter 43080/50000, Train Loss: 3.6966, LR: 0.000002, GradNorm: 0.4166, Time: 1707.4ms\n",
      "Iter 43090/50000, Train Loss: 3.7016, LR: 0.000002, GradNorm: 0.4114, Time: 1707.5ms\n",
      "Iter 43100/50000, Train Loss: 3.7622, LR: 0.000002, GradNorm: 0.4123, Time: 1706.1ms\n",
      "Iter 43110/50000, Train Loss: 3.7389, LR: 0.000002, GradNorm: 0.4093, Time: 1707.2ms\n",
      "Iter 43120/50000, Train Loss: 3.7258, LR: 0.000002, GradNorm: 0.4100, Time: 1707.0ms\n",
      "Iter 43130/50000, Train Loss: 3.7608, LR: 0.000002, GradNorm: 0.4160, Time: 1708.3ms\n",
      "Iter 43140/50000, Train Loss: 3.6865, LR: 0.000002, GradNorm: 0.4050, Time: 1706.3ms\n",
      "Iter 43150/50000, Train Loss: 3.7370, LR: 0.000002, GradNorm: 0.4109, Time: 1708.6ms\n",
      "Iter 43160/50000, Train Loss: 3.6986, LR: 0.000002, GradNorm: 0.4197, Time: 1706.7ms\n",
      "Iter 43170/50000, Train Loss: 3.7363, LR: 0.000002, GradNorm: 0.4137, Time: 1707.1ms\n",
      "Iter 43180/50000, Train Loss: 3.7854, LR: 0.000002, GradNorm: 0.4165, Time: 1707.3ms\n",
      "Iter 43190/50000, Train Loss: 3.6565, LR: 0.000002, GradNorm: 0.4070, Time: 1707.6ms\n",
      "Iter 43200/50000, Train Loss: 3.7627, LR: 0.000002, GradNorm: 0.4120, Time: 1707.2ms\n",
      "Iter 43210/50000, Train Loss: 3.7724, LR: 0.000002, GradNorm: 0.4103, Time: 1706.9ms\n",
      "Iter 43220/50000, Train Loss: 3.8098, LR: 0.000002, GradNorm: 0.4074, Time: 1707.2ms\n",
      "Iter 43230/50000, Train Loss: 3.7688, LR: 0.000002, GradNorm: 0.4121, Time: 1709.8ms\n",
      "Iter 43240/50000, Train Loss: 3.7085, LR: 0.000002, GradNorm: 0.4144, Time: 1707.4ms\n",
      "Iter 43250/50000, Train Loss: 3.7430, LR: 0.000002, GradNorm: 0.4103, Time: 1706.4ms\n",
      "Iter 43260/50000, Train Loss: 3.7922, LR: 0.000002, GradNorm: 0.4138, Time: 1707.3ms\n",
      "Iter 43270/50000, Train Loss: 3.7790, LR: 0.000002, GradNorm: 0.4243, Time: 1707.8ms\n",
      "Iter 43280/50000, Train Loss: 3.8176, LR: 0.000002, GradNorm: 0.4143, Time: 1707.1ms\n",
      "Iter 43290/50000, Train Loss: 3.6672, LR: 0.000002, GradNorm: 0.4175, Time: 1705.8ms\n",
      "Iter 43300/50000, Train Loss: 3.7944, LR: 0.000002, GradNorm: 0.4086, Time: 1707.0ms\n",
      "Iter 43310/50000, Train Loss: 3.7109, LR: 0.000002, GradNorm: 0.4108, Time: 1708.5ms\n",
      "Iter 43320/50000, Train Loss: 3.7317, LR: 0.000002, GradNorm: 0.4254, Time: 1707.6ms\n",
      "Iter 43330/50000, Train Loss: 3.7609, LR: 0.000002, GradNorm: 0.4069, Time: 1706.6ms\n",
      "Iter 43340/50000, Train Loss: 3.7359, LR: 0.000002, GradNorm: 0.4173, Time: 1706.2ms\n",
      "Iter 43350/50000, Train Loss: 3.7626, LR: 0.000002, GradNorm: 0.4092, Time: 1707.3ms\n",
      "Iter 43360/50000, Train Loss: 3.7824, LR: 0.000002, GradNorm: 0.4103, Time: 1707.6ms\n",
      "Iter 43370/50000, Train Loss: 3.7237, LR: 0.000002, GradNorm: 0.4095, Time: 1707.2ms\n",
      "Iter 43380/50000, Train Loss: 3.7329, LR: 0.000002, GradNorm: 0.4105, Time: 1707.5ms\n",
      "Iter 43390/50000, Train Loss: 3.7176, LR: 0.000002, GradNorm: 0.4050, Time: 1707.6ms\n",
      "Iter 43400/50000, Train Loss: 3.7226, LR: 0.000002, GradNorm: 0.4228, Time: 1707.1ms\n",
      "Iter 43410/50000, Train Loss: 3.7374, LR: 0.000002, GradNorm: 0.4230, Time: 1708.0ms\n",
      "Iter 43420/50000, Train Loss: 3.7320, LR: 0.000002, GradNorm: 0.4065, Time: 1706.8ms\n",
      "Iter 43430/50000, Train Loss: 3.7070, LR: 0.000002, GradNorm: 0.4129, Time: 1707.6ms\n",
      "Iter 43440/50000, Train Loss: 3.7414, LR: 0.000002, GradNorm: 0.4185, Time: 1706.5ms\n",
      "Iter 43450/50000, Train Loss: 3.7804, LR: 0.000002, GradNorm: 0.4117, Time: 1706.3ms\n",
      "Iter 43460/50000, Train Loss: 3.7700, LR: 0.000002, GradNorm: 0.4139, Time: 1707.9ms\n",
      "Iter 43470/50000, Train Loss: 3.7408, LR: 0.000002, GradNorm: 0.4296, Time: 1706.8ms\n",
      "Iter 43480/50000, Train Loss: 3.7846, LR: 0.000002, GradNorm: 0.4037, Time: 1706.6ms\n",
      "Iter 43490/50000, Train Loss: 3.8038, LR: 0.000002, GradNorm: 0.4135, Time: 1706.9ms\n",
      "Iter 43500/50000, Train Loss: 3.7245, LR: 0.000002, GradNorm: 0.4096, Time: 1707.7ms\n",
      "Iter 43510/50000, Train Loss: 3.7773, LR: 0.000002, GradNorm: 0.4087, Time: 1708.4ms\n",
      "Iter 43520/50000, Train Loss: 3.7098, LR: 0.000002, GradNorm: 0.4128, Time: 1707.4ms\n",
      "Iter 43530/50000, Train Loss: 3.6505, LR: 0.000002, GradNorm: 0.4060, Time: 1707.0ms\n",
      "Iter 43540/50000, Train Loss: 3.7743, LR: 0.000002, GradNorm: 0.4060, Time: 1707.6ms\n",
      "Iter 43550/50000, Train Loss: 3.7585, LR: 0.000002, GradNorm: 0.4101, Time: 1706.0ms\n",
      "Iter 43560/50000, Train Loss: 3.7794, LR: 0.000002, GradNorm: 0.4114, Time: 1706.7ms\n",
      "Iter 43570/50000, Train Loss: 3.6999, LR: 0.000002, GradNorm: 0.4188, Time: 1707.1ms\n",
      "Iter 43580/50000, Train Loss: 3.7803, LR: 0.000002, GradNorm: 0.4104, Time: 1708.1ms\n",
      "Iter 43590/50000, Train Loss: 3.6934, LR: 0.000002, GradNorm: 0.4175, Time: 1706.8ms\n",
      "Iter 43600/50000, Train Loss: 3.7064, LR: 0.000002, GradNorm: 0.4121, Time: 1707.3ms\n",
      "Iter 43610/50000, Train Loss: 3.6966, LR: 0.000002, GradNorm: 0.4222, Time: 1706.6ms\n",
      "Iter 43620/50000, Train Loss: 3.6983, LR: 0.000002, GradNorm: 0.4116, Time: 1706.8ms\n",
      "Iter 43630/50000, Train Loss: 3.7825, LR: 0.000002, GradNorm: 0.4206, Time: 1706.3ms\n",
      "Iter 43640/50000, Train Loss: 3.7016, LR: 0.000002, GradNorm: 0.4124, Time: 1707.6ms\n",
      "Iter 43650/50000, Train Loss: 3.7369, LR: 0.000002, GradNorm: 0.4087, Time: 1706.9ms\n",
      "Iter 43660/50000, Train Loss: 3.7567, LR: 0.000002, GradNorm: 0.4120, Time: 1706.9ms\n",
      "Iter 43670/50000, Train Loss: 3.7297, LR: 0.000002, GradNorm: 0.4105, Time: 1706.1ms\n",
      "Iter 43680/50000, Train Loss: 3.7627, LR: 0.000002, GradNorm: 0.4143, Time: 1708.0ms\n",
      "Iter 43690/50000, Train Loss: 3.7735, LR: 0.000002, GradNorm: 0.4134, Time: 1707.1ms\n",
      "Iter 43700/50000, Train Loss: 3.6957, LR: 0.000002, GradNorm: 0.4095, Time: 1706.8ms\n",
      "Iter 43710/50000, Train Loss: 3.7357, LR: 0.000002, GradNorm: 0.4142, Time: 1706.4ms\n",
      "Iter 43720/50000, Train Loss: 3.7530, LR: 0.000002, GradNorm: 0.4228, Time: 1706.3ms\n",
      "Iter 43730/50000, Train Loss: 3.7722, LR: 0.000002, GradNorm: 0.4189, Time: 1708.4ms\n",
      "Iter 43740/50000, Train Loss: 3.6958, LR: 0.000002, GradNorm: 0.4102, Time: 1706.6ms\n",
      "Iter 43750/50000, Train Loss: 3.7803, LR: 0.000002, GradNorm: 0.4230, Time: 1705.7ms\n",
      "Iter 43760/50000, Train Loss: 3.6346, LR: 0.000002, GradNorm: 0.4250, Time: 1706.7ms\n",
      "Iter 43770/50000, Train Loss: 3.7365, LR: 0.000002, GradNorm: 0.4067, Time: 1707.8ms\n",
      "Iter 43780/50000, Train Loss: 3.7503, LR: 0.000002, GradNorm: 0.4122, Time: 1707.5ms\n",
      "Iter 43790/50000, Train Loss: 3.7291, LR: 0.000002, GradNorm: 0.4217, Time: 1707.1ms\n",
      "Iter 43800/50000, Train Loss: 3.7909, LR: 0.000002, GradNorm: 0.4046, Time: 1707.0ms\n",
      "Iter 43810/50000, Train Loss: 3.7831, LR: 0.000002, GradNorm: 0.4209, Time: 1706.9ms\n",
      "Iter 43820/50000, Train Loss: 3.7523, LR: 0.000002, GradNorm: 0.4048, Time: 1707.7ms\n",
      "Iter 43830/50000, Train Loss: 3.7680, LR: 0.000002, GradNorm: 0.4147, Time: 1707.0ms\n",
      "Iter 43840/50000, Train Loss: 3.7165, LR: 0.000002, GradNorm: 0.4177, Time: 1706.3ms\n",
      "Iter 43850/50000, Train Loss: 3.7989, LR: 0.000002, GradNorm: 0.4181, Time: 1707.3ms\n",
      "Iter 43860/50000, Train Loss: 3.7397, LR: 0.000002, GradNorm: 0.4139, Time: 1707.5ms\n",
      "Iter 43870/50000, Train Loss: 3.7555, LR: 0.000002, GradNorm: 0.4252, Time: 1708.8ms\n",
      "Iter 43880/50000, Train Loss: 3.6537, LR: 0.000002, GradNorm: 0.4167, Time: 1706.5ms\n",
      "Iter 43890/50000, Train Loss: 3.7266, LR: 0.000002, GradNorm: 0.4065, Time: 1706.4ms\n",
      "Iter 43900/50000, Train Loss: 3.6925, LR: 0.000002, GradNorm: 0.4157, Time: 1707.3ms\n",
      "Iter 43910/50000, Train Loss: 3.7218, LR: 0.000002, GradNorm: 0.4126, Time: 1707.0ms\n",
      "Iter 43920/50000, Train Loss: 3.7481, LR: 0.000002, GradNorm: 0.4145, Time: 1706.7ms\n",
      "Iter 43930/50000, Train Loss: 3.6916, LR: 0.000002, GradNorm: 0.4196, Time: 1707.1ms\n",
      "Iter 43940/50000, Train Loss: 3.7054, LR: 0.000002, GradNorm: 0.4143, Time: 1706.2ms\n",
      "Iter 43950/50000, Train Loss: 3.7553, LR: 0.000002, GradNorm: 0.4057, Time: 1706.8ms\n",
      "Iter 43960/50000, Train Loss: 3.7476, LR: 0.000002, GradNorm: 0.4072, Time: 1708.3ms\n",
      "Iter 43970/50000, Train Loss: 3.7475, LR: 0.000002, GradNorm: 0.4170, Time: 1706.9ms\n",
      "Iter 43980/50000, Train Loss: 3.7099, LR: 0.000002, GradNorm: 0.4147, Time: 1707.5ms\n",
      "Iter 43990/50000, Train Loss: 3.6942, LR: 0.000002, GradNorm: 0.4251, Time: 1707.6ms\n",
      "Iter 44000/50000, Train Loss: 3.7744, LR: 0.000002, GradNorm: 0.4128, Time: 1707.3ms\n",
      "Iter 44010/50000, Train Loss: 3.7623, LR: 0.000002, GradNorm: 0.4130, Time: 1707.5ms\n",
      "Iter 44020/50000, Train Loss: 3.7094, LR: 0.000002, GradNorm: 0.4147, Time: 1706.4ms\n",
      "Iter 44030/50000, Train Loss: 3.7476, LR: 0.000002, GradNorm: 0.4204, Time: 1706.9ms\n",
      "Iter 44040/50000, Train Loss: 3.7814, LR: 0.000002, GradNorm: 0.4248, Time: 1708.6ms\n",
      "Iter 44050/50000, Train Loss: 3.7589, LR: 0.000002, GradNorm: 0.4145, Time: 1708.0ms\n",
      "Iter 44060/50000, Train Loss: 3.7520, LR: 0.000002, GradNorm: 0.4091, Time: 1706.7ms\n",
      "Iter 44070/50000, Train Loss: 3.8002, LR: 0.000002, GradNorm: 0.4086, Time: 1707.3ms\n",
      "Iter 44080/50000, Train Loss: 3.6922, LR: 0.000002, GradNorm: 0.4202, Time: 1706.8ms\n",
      "Iter 44090/50000, Train Loss: 3.7228, LR: 0.000002, GradNorm: 0.4192, Time: 1707.4ms\n",
      "Iter 44100/50000, Train Loss: 3.7842, LR: 0.000002, GradNorm: 0.4111, Time: 1706.2ms\n",
      "Iter 44110/50000, Train Loss: 3.7597, LR: 0.000002, GradNorm: 0.4240, Time: 1706.8ms\n",
      "Iter 44120/50000, Train Loss: 3.7585, LR: 0.000002, GradNorm: 0.4110, Time: 1706.3ms\n",
      "Iter 44130/50000, Train Loss: 3.7420, LR: 0.000002, GradNorm: 0.4115, Time: 1706.1ms\n",
      "Iter 44140/50000, Train Loss: 3.7848, LR: 0.000002, GradNorm: 0.4026, Time: 1706.5ms\n",
      "Iter 44150/50000, Train Loss: 3.7535, LR: 0.000002, GradNorm: 0.4101, Time: 1707.7ms\n",
      "Iter 44160/50000, Train Loss: 3.7504, LR: 0.000002, GradNorm: 0.4093, Time: 1706.6ms\n",
      "Iter 44170/50000, Train Loss: 3.7252, LR: 0.000002, GradNorm: 0.4135, Time: 1706.4ms\n",
      "Iter 44180/50000, Train Loss: 3.7229, LR: 0.000002, GradNorm: 0.4226, Time: 1707.0ms\n",
      "Iter 44190/50000, Train Loss: 3.7790, LR: 0.000002, GradNorm: 0.4115, Time: 1708.0ms\n",
      "Iter 44200/50000, Train Loss: 3.8011, LR: 0.000002, GradNorm: 0.4125, Time: 1706.3ms\n",
      "Iter 44210/50000, Train Loss: 3.7688, LR: 0.000002, GradNorm: 0.4178, Time: 1706.3ms\n",
      "Iter 44220/50000, Train Loss: 3.8003, LR: 0.000002, GradNorm: 0.4129, Time: 1707.0ms\n",
      "Iter 44230/50000, Train Loss: 3.7081, LR: 0.000002, GradNorm: 0.4169, Time: 1707.7ms\n",
      "Iter 44240/50000, Train Loss: 3.7404, LR: 0.000002, GradNorm: 0.4116, Time: 1706.6ms\n",
      "Iter 44250/50000, Train Loss: 3.7874, LR: 0.000002, GradNorm: 0.4108, Time: 1706.4ms\n",
      "Iter 44260/50000, Train Loss: 3.7266, LR: 0.000002, GradNorm: 0.4118, Time: 1707.1ms\n",
      "Iter 44270/50000, Train Loss: 3.7938, LR: 0.000002, GradNorm: 0.4097, Time: 1707.4ms\n",
      "Iter 44280/50000, Train Loss: 3.7639, LR: 0.000002, GradNorm: 0.4224, Time: 1706.7ms\n",
      "Iter 44290/50000, Train Loss: 3.7980, LR: 0.000002, GradNorm: 0.4139, Time: 1707.3ms\n",
      "Iter 44300/50000, Train Loss: 3.7127, LR: 0.000002, GradNorm: 0.4108, Time: 1706.9ms\n",
      "Iter 44310/50000, Train Loss: 3.7491, LR: 0.000002, GradNorm: 0.4122, Time: 1706.3ms\n",
      "Iter 44320/50000, Train Loss: 3.7359, LR: 0.000002, GradNorm: 0.4123, Time: 1706.6ms\n",
      "Iter 44330/50000, Train Loss: 3.7730, LR: 0.000002, GradNorm: 0.4075, Time: 1708.1ms\n",
      "Iter 44340/50000, Train Loss: 3.7706, LR: 0.000002, GradNorm: 0.4119, Time: 1707.6ms\n",
      "Iter 44350/50000, Train Loss: 3.7784, LR: 0.000002, GradNorm: 0.4177, Time: 1707.0ms\n",
      "Iter 44360/50000, Train Loss: 3.7739, LR: 0.000002, GradNorm: 0.4079, Time: 1706.0ms\n",
      "Iter 44370/50000, Train Loss: 3.7942, LR: 0.000002, GradNorm: 0.4109, Time: 1706.2ms\n",
      "Iter 44380/50000, Train Loss: 3.7381, LR: 0.000002, GradNorm: 0.4038, Time: 1707.4ms\n",
      "Iter 44390/50000, Train Loss: 3.7459, LR: 0.000002, GradNorm: 0.4095, Time: 1707.2ms\n",
      "Iter 44400/50000, Train Loss: 3.7644, LR: 0.000002, GradNorm: 0.4087, Time: 1705.5ms\n",
      "Iter 44410/50000, Train Loss: 3.7234, LR: 0.000002, GradNorm: 0.4157, Time: 1707.3ms\n",
      "Iter 44420/50000, Train Loss: 3.7812, LR: 0.000002, GradNorm: 0.4068, Time: 1708.1ms\n",
      "Iter 44430/50000, Train Loss: 3.7371, LR: 0.000002, GradNorm: 0.4225, Time: 1707.6ms\n",
      "Iter 44440/50000, Train Loss: 3.7041, LR: 0.000002, GradNorm: 0.4152, Time: 1707.0ms\n",
      "Iter 44450/50000, Train Loss: 3.7848, LR: 0.000002, GradNorm: 0.4192, Time: 1706.2ms\n",
      "Iter 44460/50000, Train Loss: 3.7644, LR: 0.000002, GradNorm: 0.4220, Time: 1707.0ms\n",
      "Iter 44470/50000, Train Loss: 3.7318, LR: 0.000002, GradNorm: 0.4178, Time: 1708.0ms\n",
      "Iter 44480/50000, Train Loss: 3.7108, LR: 0.000002, GradNorm: 0.4062, Time: 1706.3ms\n",
      "Iter 44490/50000, Train Loss: 3.8178, LR: 0.000002, GradNorm: 0.4233, Time: 1706.2ms\n",
      "Iter 44500/50000, Train Loss: 3.7255, LR: 0.000002, GradNorm: 0.4113, Time: 1706.9ms\n",
      "Iter 44510/50000, Train Loss: 3.7659, LR: 0.000002, GradNorm: 0.4109, Time: 1706.9ms\n",
      "Iter 44520/50000, Train Loss: 3.8027, LR: 0.000002, GradNorm: 0.4189, Time: 1706.7ms\n",
      "Iter 44530/50000, Train Loss: 3.7055, LR: 0.000002, GradNorm: 0.4219, Time: 1706.6ms\n",
      "Iter 44540/50000, Train Loss: 3.7179, LR: 0.000002, GradNorm: 0.4134, Time: 1706.9ms\n",
      "Iter 44550/50000, Train Loss: 3.7599, LR: 0.000002, GradNorm: 0.4244, Time: 1706.9ms\n",
      "Iter 44560/50000, Train Loss: 3.7472, LR: 0.000002, GradNorm: 0.4102, Time: 1705.7ms\n",
      "Iter 44570/50000, Train Loss: 3.7693, LR: 0.000002, GradNorm: 0.4147, Time: 1708.1ms\n",
      "Iter 44580/50000, Train Loss: 3.7677, LR: 0.000002, GradNorm: 0.4192, Time: 1707.0ms\n",
      "Iter 44590/50000, Train Loss: 3.7746, LR: 0.000002, GradNorm: 0.4131, Time: 1705.9ms\n",
      "Iter 44600/50000, Train Loss: 3.7523, LR: 0.000002, GradNorm: 0.4103, Time: 1707.7ms\n",
      "Iter 44610/50000, Train Loss: 3.7656, LR: 0.000002, GradNorm: 0.4101, Time: 1708.3ms\n",
      "Iter 44620/50000, Train Loss: 3.7262, LR: 0.000002, GradNorm: 0.4193, Time: 1706.9ms\n",
      "Iter 44630/50000, Train Loss: 3.7616, LR: 0.000002, GradNorm: 0.4138, Time: 1707.5ms\n",
      "Iter 44640/50000, Train Loss: 3.7845, LR: 0.000002, GradNorm: 0.4090, Time: 1706.5ms\n",
      "Iter 44650/50000, Train Loss: 3.7506, LR: 0.000002, GradNorm: 0.4115, Time: 1707.7ms\n",
      "Iter 44660/50000, Train Loss: 3.7663, LR: 0.000002, GradNorm: 0.4238, Time: 1706.8ms\n",
      "Iter 44670/50000, Train Loss: 3.7269, LR: 0.000002, GradNorm: 0.4173, Time: 1706.6ms\n",
      "Iter 44680/50000, Train Loss: 3.7259, LR: 0.000002, GradNorm: 0.4025, Time: 1707.1ms\n",
      "Iter 44690/50000, Train Loss: 3.7589, LR: 0.000002, GradNorm: 0.4120, Time: 1708.9ms\n",
      "Iter 44700/50000, Train Loss: 3.8191, LR: 0.000002, GradNorm: 0.4181, Time: 1707.7ms\n",
      "Iter 44710/50000, Train Loss: 3.7249, LR: 0.000002, GradNorm: 0.4231, Time: 1707.0ms\n",
      "Iter 44720/50000, Train Loss: 3.7275, LR: 0.000002, GradNorm: 0.4262, Time: 1706.3ms\n",
      "Iter 44730/50000, Train Loss: 3.7570, LR: 0.000002, GradNorm: 0.4111, Time: 1707.1ms\n",
      "Iter 44740/50000, Train Loss: 3.7843, LR: 0.000002, GradNorm: 0.4144, Time: 1706.7ms\n",
      "Iter 44750/50000, Train Loss: 3.7580, LR: 0.000002, GradNorm: 0.4094, Time: 1707.9ms\n",
      "Iter 44760/50000, Train Loss: 3.7143, LR: 0.000002, GradNorm: 0.4095, Time: 1705.8ms\n",
      "Iter 44770/50000, Train Loss: 3.7042, LR: 0.000002, GradNorm: 0.4073, Time: 1707.7ms\n",
      "Iter 44780/50000, Train Loss: 3.7506, LR: 0.000002, GradNorm: 0.4135, Time: 1706.6ms\n",
      "Iter 44790/50000, Train Loss: 3.7680, LR: 0.000002, GradNorm: 0.4137, Time: 1709.0ms\n",
      "Iter 44800/50000, Train Loss: 3.6906, LR: 0.000002, GradNorm: 0.4163, Time: 1707.6ms\n",
      "Iter 44810/50000, Train Loss: 3.7584, LR: 0.000002, GradNorm: 0.4092, Time: 1705.9ms\n",
      "Iter 44820/50000, Train Loss: 3.7236, LR: 0.000002, GradNorm: 0.4165, Time: 1707.6ms\n",
      "Iter 44830/50000, Train Loss: 3.8099, LR: 0.000002, GradNorm: 0.4229, Time: 1707.0ms\n",
      "Iter 44840/50000, Train Loss: 3.7789, LR: 0.000002, GradNorm: 0.4108, Time: 1707.9ms\n",
      "Iter 44850/50000, Train Loss: 3.7936, LR: 0.000002, GradNorm: 0.4096, Time: 1706.2ms\n",
      "Iter 44860/50000, Train Loss: 3.7346, LR: 0.000002, GradNorm: 0.4259, Time: 1706.3ms\n",
      "Iter 44870/50000, Train Loss: 3.7590, LR: 0.000002, GradNorm: 0.4127, Time: 1708.2ms\n",
      "Iter 44880/50000, Train Loss: 3.7575, LR: 0.000002, GradNorm: 0.4046, Time: 1708.6ms\n",
      "Iter 44890/50000, Train Loss: 3.6761, LR: 0.000002, GradNorm: 0.4059, Time: 1707.7ms\n",
      "Iter 44900/50000, Train Loss: 3.6876, LR: 0.000002, GradNorm: 0.4103, Time: 1706.4ms\n",
      "Iter 44910/50000, Train Loss: 3.7358, LR: 0.000002, GradNorm: 0.4009, Time: 1706.2ms\n",
      "Iter 44920/50000, Train Loss: 3.6969, LR: 0.000002, GradNorm: 0.4141, Time: 1706.7ms\n",
      "Iter 44930/50000, Train Loss: 3.7508, LR: 0.000002, GradNorm: 0.4061, Time: 1707.8ms\n",
      "Iter 44940/50000, Train Loss: 3.7387, LR: 0.000002, GradNorm: 0.4179, Time: 1706.9ms\n",
      "Iter 44950/50000, Train Loss: 3.7408, LR: 0.000002, GradNorm: 0.4057, Time: 1706.2ms\n",
      "Iter 44960/50000, Train Loss: 3.6681, LR: 0.000002, GradNorm: 0.4081, Time: 1708.3ms\n",
      "Iter 44970/50000, Train Loss: 3.6581, LR: 0.000002, GradNorm: 0.4099, Time: 1706.7ms\n",
      "Iter 44980/50000, Train Loss: 3.7582, LR: 0.000002, GradNorm: 0.4082, Time: 1707.8ms\n",
      "Iter 44990/50000, Train Loss: 3.7456, LR: 0.000002, GradNorm: 0.4149, Time: 1706.9ms\n",
      "Iter 45000/50000, Train Loss: 3.7602, LR: 0.000002, GradNorm: 0.4204, Time: 1707.1ms\n",
      "--- Eval at iter 45000: Val Loss: 3.7441 ---\n",
      "save checkpoints/model_iter_45000.pt iterations: 45000\n",
      "Iter 45010/50000, Train Loss: 3.7292, LR: 0.000002, GradNorm: 0.4272, Time: 11583.0ms\n",
      "Iter 45020/50000, Train Loss: 3.7059, LR: 0.000002, GradNorm: 0.4121, Time: 1706.2ms\n",
      "Iter 45030/50000, Train Loss: 3.6726, LR: 0.000002, GradNorm: 0.4236, Time: 1705.6ms\n",
      "Iter 45040/50000, Train Loss: 3.7283, LR: 0.000002, GradNorm: 0.4133, Time: 1706.3ms\n",
      "Iter 45050/50000, Train Loss: 3.8259, LR: 0.000002, GradNorm: 0.4128, Time: 1705.3ms\n",
      "Iter 45060/50000, Train Loss: 3.7631, LR: 0.000002, GradNorm: 0.4131, Time: 1706.1ms\n",
      "Iter 45070/50000, Train Loss: 3.7381, LR: 0.000002, GradNorm: 0.4236, Time: 1706.0ms\n",
      "Iter 45080/50000, Train Loss: 3.7163, LR: 0.000002, GradNorm: 0.4140, Time: 1706.1ms\n",
      "Iter 45090/50000, Train Loss: 3.7352, LR: 0.000002, GradNorm: 0.4121, Time: 1706.6ms\n",
      "Iter 45100/50000, Train Loss: 3.7264, LR: 0.000002, GradNorm: 0.4161, Time: 1708.2ms\n",
      "Iter 45110/50000, Train Loss: 3.7057, LR: 0.000002, GradNorm: 0.4048, Time: 1706.3ms\n",
      "Iter 45120/50000, Train Loss: 3.7509, LR: 0.000002, GradNorm: 0.4194, Time: 1705.8ms\n",
      "Iter 45130/50000, Train Loss: 3.7259, LR: 0.000002, GradNorm: 0.4088, Time: 1706.3ms\n",
      "Iter 45140/50000, Train Loss: 3.7366, LR: 0.000002, GradNorm: 0.4123, Time: 1707.2ms\n",
      "Iter 45150/50000, Train Loss: 3.7893, LR: 0.000002, GradNorm: 0.4110, Time: 1706.3ms\n",
      "Iter 45160/50000, Train Loss: 3.7418, LR: 0.000002, GradNorm: 0.4104, Time: 1706.8ms\n",
      "Iter 45170/50000, Train Loss: 3.7796, LR: 0.000002, GradNorm: 0.4077, Time: 1706.5ms\n",
      "Iter 45180/50000, Train Loss: 3.7672, LR: 0.000002, GradNorm: 0.4122, Time: 1706.7ms\n",
      "Iter 45190/50000, Train Loss: 3.7421, LR: 0.000002, GradNorm: 0.4161, Time: 1707.5ms\n",
      "Iter 45200/50000, Train Loss: 3.7868, LR: 0.000002, GradNorm: 0.4199, Time: 1707.0ms\n",
      "Iter 45210/50000, Train Loss: 3.7631, LR: 0.000002, GradNorm: 0.4128, Time: 1706.7ms\n",
      "Iter 45220/50000, Train Loss: 3.7528, LR: 0.000002, GradNorm: 0.4090, Time: 1707.8ms\n",
      "Iter 45230/50000, Train Loss: 3.7140, LR: 0.000002, GradNorm: 0.4196, Time: 1706.6ms\n",
      "Iter 45240/50000, Train Loss: 3.7111, LR: 0.000002, GradNorm: 0.4057, Time: 1706.5ms\n",
      "Iter 45250/50000, Train Loss: 3.7501, LR: 0.000002, GradNorm: 0.4136, Time: 1707.7ms\n",
      "Iter 45260/50000, Train Loss: 3.6495, LR: 0.000002, GradNorm: 0.4137, Time: 1706.0ms\n",
      "Iter 45270/50000, Train Loss: 3.7598, LR: 0.000002, GradNorm: 0.4204, Time: 1706.7ms\n",
      "Iter 45280/50000, Train Loss: 3.7566, LR: 0.000002, GradNorm: 0.4206, Time: 1706.6ms\n",
      "Iter 45290/50000, Train Loss: 3.7728, LR: 0.000002, GradNorm: 0.4149, Time: 1708.0ms\n",
      "Iter 45300/50000, Train Loss: 3.7098, LR: 0.000002, GradNorm: 0.4043, Time: 1707.0ms\n",
      "Iter 45310/50000, Train Loss: 3.7330, LR: 0.000002, GradNorm: 0.4079, Time: 1706.2ms\n",
      "Iter 45320/50000, Train Loss: 3.7377, LR: 0.000002, GradNorm: 0.4258, Time: 1706.9ms\n",
      "Iter 45330/50000, Train Loss: 3.7198, LR: 0.000002, GradNorm: 0.4143, Time: 1706.2ms\n",
      "Iter 45340/50000, Train Loss: 3.7582, LR: 0.000002, GradNorm: 0.4189, Time: 1706.4ms\n",
      "Iter 45350/50000, Train Loss: 3.7567, LR: 0.000002, GradNorm: 0.4172, Time: 1706.3ms\n",
      "Iter 45360/50000, Train Loss: 3.7266, LR: 0.000002, GradNorm: 0.4098, Time: 1706.6ms\n",
      "Iter 45370/50000, Train Loss: 3.7777, LR: 0.000002, GradNorm: 0.4165, Time: 1705.7ms\n",
      "Iter 45380/50000, Train Loss: 3.7387, LR: 0.000002, GradNorm: 0.4208, Time: 1707.7ms\n",
      "Iter 45390/50000, Train Loss: 3.7925, LR: 0.000002, GradNorm: 0.4122, Time: 1706.5ms\n",
      "Iter 45400/50000, Train Loss: 3.7240, LR: 0.000002, GradNorm: 0.4111, Time: 1707.2ms\n",
      "Iter 45410/50000, Train Loss: 3.7440, LR: 0.000002, GradNorm: 0.4131, Time: 1706.9ms\n",
      "Iter 45420/50000, Train Loss: 3.7583, LR: 0.000002, GradNorm: 0.4087, Time: 1706.1ms\n",
      "Iter 45430/50000, Train Loss: 3.7659, LR: 0.000002, GradNorm: 0.4178, Time: 1708.2ms\n",
      "Iter 45440/50000, Train Loss: 3.7372, LR: 0.000002, GradNorm: 0.4142, Time: 1707.0ms\n",
      "Iter 45450/50000, Train Loss: 3.7461, LR: 0.000002, GradNorm: 0.4160, Time: 1706.7ms\n",
      "Iter 45460/50000, Train Loss: 3.7276, LR: 0.000002, GradNorm: 0.4198, Time: 1708.8ms\n",
      "Iter 45470/50000, Train Loss: 3.6989, LR: 0.000002, GradNorm: 0.4130, Time: 1707.4ms\n",
      "Iter 45480/50000, Train Loss: 3.7182, LR: 0.000002, GradNorm: 0.4023, Time: 1707.7ms\n",
      "Iter 45490/50000, Train Loss: 3.6856, LR: 0.000002, GradNorm: 0.4122, Time: 1706.7ms\n",
      "Iter 45500/50000, Train Loss: 3.7225, LR: 0.000002, GradNorm: 0.4180, Time: 1709.1ms\n",
      "Iter 45510/50000, Train Loss: 3.7152, LR: 0.000002, GradNorm: 0.4117, Time: 1710.5ms\n",
      "Iter 45520/50000, Train Loss: 3.7635, LR: 0.000002, GradNorm: 0.4086, Time: 1709.0ms\n",
      "Iter 45530/50000, Train Loss: 3.7804, LR: 0.000002, GradNorm: 0.4214, Time: 1708.7ms\n",
      "Iter 45540/50000, Train Loss: 3.6840, LR: 0.000002, GradNorm: 0.4064, Time: 1708.6ms\n",
      "Iter 45550/50000, Train Loss: 3.7308, LR: 0.000002, GradNorm: 0.4098, Time: 1710.8ms\n",
      "Iter 45560/50000, Train Loss: 3.7634, LR: 0.000002, GradNorm: 0.4121, Time: 1708.4ms\n",
      "Iter 45570/50000, Train Loss: 3.7506, LR: 0.000002, GradNorm: 0.4361, Time: 1709.9ms\n",
      "Iter 45580/50000, Train Loss: 3.7403, LR: 0.000002, GradNorm: 0.4133, Time: 1709.7ms\n",
      "Iter 45590/50000, Train Loss: 3.7222, LR: 0.000002, GradNorm: 0.4313, Time: 1708.1ms\n",
      "Iter 45600/50000, Train Loss: 3.7512, LR: 0.000002, GradNorm: 0.4142, Time: 1708.9ms\n",
      "Iter 45610/50000, Train Loss: 3.7609, LR: 0.000002, GradNorm: 0.4253, Time: 1708.7ms\n",
      "Iter 45620/50000, Train Loss: 3.6903, LR: 0.000002, GradNorm: 0.4144, Time: 1708.0ms\n",
      "Iter 45630/50000, Train Loss: 3.7533, LR: 0.000002, GradNorm: 0.4182, Time: 1709.4ms\n",
      "Iter 45640/50000, Train Loss: 3.7087, LR: 0.000002, GradNorm: 0.4122, Time: 1710.4ms\n",
      "Iter 45650/50000, Train Loss: 3.7348, LR: 0.000002, GradNorm: 0.4221, Time: 1709.7ms\n",
      "Iter 45660/50000, Train Loss: 3.8035, LR: 0.000002, GradNorm: 0.4194, Time: 1708.7ms\n",
      "Iter 45670/50000, Train Loss: 3.7421, LR: 0.000002, GradNorm: 0.4113, Time: 1709.8ms\n",
      "Iter 45680/50000, Train Loss: 3.7188, LR: 0.000002, GradNorm: 0.4079, Time: 1710.6ms\n",
      "Iter 45690/50000, Train Loss: 3.7588, LR: 0.000002, GradNorm: 0.4051, Time: 1709.8ms\n",
      "Iter 45700/50000, Train Loss: 3.8100, LR: 0.000002, GradNorm: 0.4333, Time: 1709.7ms\n",
      "Iter 45710/50000, Train Loss: 3.7384, LR: 0.000002, GradNorm: 0.4158, Time: 1709.8ms\n",
      "Iter 45720/50000, Train Loss: 3.7977, LR: 0.000002, GradNorm: 0.4132, Time: 1710.8ms\n",
      "Iter 45730/50000, Train Loss: 3.7659, LR: 0.000002, GradNorm: 0.4092, Time: 1709.3ms\n",
      "Iter 45740/50000, Train Loss: 3.7242, LR: 0.000002, GradNorm: 0.4169, Time: 1709.2ms\n",
      "Iter 45750/50000, Train Loss: 3.7203, LR: 0.000002, GradNorm: 0.4093, Time: 1709.8ms\n",
      "Iter 45760/50000, Train Loss: 3.7948, LR: 0.000002, GradNorm: 0.4202, Time: 1708.6ms\n",
      "Iter 45770/50000, Train Loss: 3.7657, LR: 0.000002, GradNorm: 0.4155, Time: 1710.8ms\n",
      "Iter 45780/50000, Train Loss: 3.7332, LR: 0.000002, GradNorm: 0.4092, Time: 1708.5ms\n",
      "Iter 45790/50000, Train Loss: 3.7543, LR: 0.000002, GradNorm: 0.4121, Time: 1709.2ms\n",
      "Iter 45800/50000, Train Loss: 3.7239, LR: 0.000002, GradNorm: 0.4086, Time: 1709.1ms\n",
      "Iter 45810/50000, Train Loss: 3.7529, LR: 0.000002, GradNorm: 0.4243, Time: 1710.1ms\n",
      "Iter 45820/50000, Train Loss: 3.8369, LR: 0.000002, GradNorm: 0.4111, Time: 1709.2ms\n",
      "Iter 45830/50000, Train Loss: 3.7195, LR: 0.000002, GradNorm: 0.4109, Time: 1708.8ms\n",
      "Iter 45840/50000, Train Loss: 3.7433, LR: 0.000002, GradNorm: 0.4187, Time: 1709.6ms\n",
      "Iter 45850/50000, Train Loss: 3.7140, LR: 0.000002, GradNorm: 0.4060, Time: 1709.7ms\n",
      "Iter 45860/50000, Train Loss: 3.7408, LR: 0.000002, GradNorm: 0.4135, Time: 1708.5ms\n",
      "Iter 45870/50000, Train Loss: 3.7319, LR: 0.000002, GradNorm: 0.4045, Time: 1708.8ms\n",
      "Iter 45880/50000, Train Loss: 3.6991, LR: 0.000002, GradNorm: 0.4084, Time: 1709.7ms\n",
      "Iter 45890/50000, Train Loss: 3.7095, LR: 0.000002, GradNorm: 0.4079, Time: 1711.8ms\n",
      "Iter 45900/50000, Train Loss: 3.6991, LR: 0.000002, GradNorm: 0.4141, Time: 1710.5ms\n",
      "Iter 45910/50000, Train Loss: 3.7185, LR: 0.000002, GradNorm: 0.4258, Time: 1709.4ms\n",
      "Iter 45920/50000, Train Loss: 3.7124, LR: 0.000002, GradNorm: 0.4145, Time: 1709.5ms\n",
      "Iter 45930/50000, Train Loss: 3.7455, LR: 0.000002, GradNorm: 0.4128, Time: 1709.8ms\n",
      "Iter 45940/50000, Train Loss: 3.7527, LR: 0.000002, GradNorm: 0.4231, Time: 1710.8ms\n",
      "Iter 45950/50000, Train Loss: 3.7446, LR: 0.000002, GradNorm: 0.4313, Time: 1711.1ms\n",
      "Iter 45960/50000, Train Loss: 3.7979, LR: 0.000002, GradNorm: 0.4220, Time: 1710.1ms\n",
      "Iter 45970/50000, Train Loss: 3.7736, LR: 0.000002, GradNorm: 0.4127, Time: 1710.2ms\n",
      "Iter 45980/50000, Train Loss: 3.7048, LR: 0.000002, GradNorm: 0.4187, Time: 1711.0ms\n",
      "Iter 45990/50000, Train Loss: 3.7142, LR: 0.000002, GradNorm: 0.4121, Time: 1709.8ms\n",
      "Iter 46000/50000, Train Loss: 3.7487, LR: 0.000002, GradNorm: 0.4183, Time: 1710.9ms\n",
      "Iter 46010/50000, Train Loss: 3.7063, LR: 0.000002, GradNorm: 0.4132, Time: 1709.6ms\n",
      "Iter 46020/50000, Train Loss: 3.7783, LR: 0.000002, GradNorm: 0.4252, Time: 1710.6ms\n",
      "Iter 46030/50000, Train Loss: 3.7181, LR: 0.000002, GradNorm: 0.4066, Time: 1708.8ms\n",
      "Iter 46040/50000, Train Loss: 3.7786, LR: 0.000002, GradNorm: 0.4123, Time: 1709.1ms\n",
      "Iter 46050/50000, Train Loss: 3.7629, LR: 0.000002, GradNorm: 0.4179, Time: 1709.9ms\n",
      "Iter 46060/50000, Train Loss: 3.7288, LR: 0.000002, GradNorm: 0.4187, Time: 1710.9ms\n",
      "Iter 46070/50000, Train Loss: 3.7438, LR: 0.000002, GradNorm: 0.4133, Time: 1709.8ms\n",
      "Iter 46080/50000, Train Loss: 3.8120, LR: 0.000002, GradNorm: 0.4141, Time: 1709.4ms\n",
      "Iter 46090/50000, Train Loss: 3.7417, LR: 0.000002, GradNorm: 0.4237, Time: 1710.8ms\n",
      "Iter 46100/50000, Train Loss: 3.7639, LR: 0.000002, GradNorm: 0.4136, Time: 1710.0ms\n",
      "Iter 46110/50000, Train Loss: 3.7034, LR: 0.000002, GradNorm: 0.4255, Time: 1710.6ms\n",
      "Iter 46120/50000, Train Loss: 3.7966, LR: 0.000002, GradNorm: 0.4243, Time: 1709.8ms\n",
      "Iter 46130/50000, Train Loss: 3.7296, LR: 0.000002, GradNorm: 0.4127, Time: 1708.7ms\n",
      "Iter 46140/50000, Train Loss: 3.7286, LR: 0.000002, GradNorm: 0.4174, Time: 1709.5ms\n",
      "Iter 46150/50000, Train Loss: 3.7199, LR: 0.000002, GradNorm: 0.4072, Time: 1709.7ms\n",
      "Iter 46160/50000, Train Loss: 3.7887, LR: 0.000002, GradNorm: 0.4191, Time: 1710.0ms\n",
      "Iter 46170/50000, Train Loss: 3.7605, LR: 0.000002, GradNorm: 0.4165, Time: 1709.0ms\n",
      "Iter 46180/50000, Train Loss: 3.7802, LR: 0.000002, GradNorm: 0.4144, Time: 1709.8ms\n",
      "Iter 46190/50000, Train Loss: 3.7362, LR: 0.000002, GradNorm: 0.4104, Time: 1710.9ms\n",
      "Iter 46200/50000, Train Loss: 3.7454, LR: 0.000002, GradNorm: 0.4133, Time: 1710.4ms\n",
      "Iter 46210/50000, Train Loss: 3.7531, LR: 0.000002, GradNorm: 0.4086, Time: 1709.7ms\n",
      "Iter 46220/50000, Train Loss: 3.7447, LR: 0.000002, GradNorm: 0.4098, Time: 1710.6ms\n",
      "Iter 46230/50000, Train Loss: 3.7419, LR: 0.000002, GradNorm: 0.4105, Time: 1711.9ms\n",
      "Iter 46240/50000, Train Loss: 3.7861, LR: 0.000002, GradNorm: 0.4236, Time: 1710.2ms\n",
      "Iter 46250/50000, Train Loss: 3.7464, LR: 0.000002, GradNorm: 0.4192, Time: 1710.2ms\n",
      "Iter 46260/50000, Train Loss: 3.7452, LR: 0.000002, GradNorm: 0.4141, Time: 1710.4ms\n",
      "Iter 46270/50000, Train Loss: 3.7481, LR: 0.000002, GradNorm: 0.4197, Time: 1710.0ms\n",
      "Iter 46280/50000, Train Loss: 3.7514, LR: 0.000002, GradNorm: 0.4105, Time: 1710.2ms\n",
      "Iter 46290/50000, Train Loss: 3.7097, LR: 0.000002, GradNorm: 0.4088, Time: 1711.1ms\n",
      "Iter 46300/50000, Train Loss: 3.7631, LR: 0.000002, GradNorm: 0.4372, Time: 1710.5ms\n",
      "Iter 46310/50000, Train Loss: 3.7268, LR: 0.000002, GradNorm: 0.4121, Time: 1710.7ms\n",
      "Iter 46320/50000, Train Loss: 3.7394, LR: 0.000002, GradNorm: 0.4096, Time: 1712.0ms\n",
      "Iter 46330/50000, Train Loss: 3.7569, LR: 0.000002, GradNorm: 0.4091, Time: 1709.7ms\n",
      "Iter 46340/50000, Train Loss: 3.7196, LR: 0.000002, GradNorm: 0.4264, Time: 1710.1ms\n",
      "Iter 46350/50000, Train Loss: 3.7698, LR: 0.000002, GradNorm: 0.4164, Time: 1709.7ms\n",
      "Iter 46360/50000, Train Loss: 3.7410, LR: 0.000002, GradNorm: 0.4220, Time: 1711.2ms\n",
      "Iter 46370/50000, Train Loss: 3.7729, LR: 0.000002, GradNorm: 0.4151, Time: 1709.7ms\n",
      "Iter 46380/50000, Train Loss: 3.7310, LR: 0.000002, GradNorm: 0.4106, Time: 1711.3ms\n",
      "Iter 46390/50000, Train Loss: 3.7583, LR: 0.000002, GradNorm: 0.4127, Time: 1711.1ms\n",
      "Iter 46400/50000, Train Loss: 3.7497, LR: 0.000002, GradNorm: 0.4123, Time: 1711.4ms\n",
      "Iter 46410/50000, Train Loss: 3.7546, LR: 0.000002, GradNorm: 0.4121, Time: 1711.1ms\n",
      "Iter 46420/50000, Train Loss: 3.7674, LR: 0.000002, GradNorm: 0.4177, Time: 1710.5ms\n",
      "Iter 46430/50000, Train Loss: 3.7740, LR: 0.000002, GradNorm: 0.4140, Time: 1710.9ms\n",
      "Iter 46440/50000, Train Loss: 3.7285, LR: 0.000002, GradNorm: 0.4110, Time: 1710.2ms\n",
      "Iter 46450/50000, Train Loss: 3.7438, LR: 0.000002, GradNorm: 0.4084, Time: 1710.1ms\n",
      "Iter 46460/50000, Train Loss: 3.7842, LR: 0.000002, GradNorm: 0.4156, Time: 1710.0ms\n",
      "Iter 46470/50000, Train Loss: 3.7291, LR: 0.000002, GradNorm: 0.4152, Time: 1709.6ms\n",
      "Iter 46480/50000, Train Loss: 3.7452, LR: 0.000002, GradNorm: 0.4070, Time: 1710.2ms\n",
      "Iter 46490/50000, Train Loss: 3.7161, LR: 0.000002, GradNorm: 0.4162, Time: 1711.5ms\n",
      "Iter 46500/50000, Train Loss: 3.6723, LR: 0.000002, GradNorm: 0.4147, Time: 1710.7ms\n",
      "Iter 46510/50000, Train Loss: 3.7972, LR: 0.000002, GradNorm: 0.4182, Time: 1710.3ms\n",
      "Iter 46520/50000, Train Loss: 3.7205, LR: 0.000002, GradNorm: 0.4148, Time: 1710.7ms\n",
      "Iter 46530/50000, Train Loss: 3.7976, LR: 0.000002, GradNorm: 0.4140, Time: 1711.1ms\n",
      "Iter 46540/50000, Train Loss: 3.7147, LR: 0.000002, GradNorm: 0.4096, Time: 1710.0ms\n",
      "Iter 46550/50000, Train Loss: 3.7919, LR: 0.000002, GradNorm: 0.4193, Time: 1709.2ms\n",
      "Iter 46560/50000, Train Loss: 3.7332, LR: 0.000002, GradNorm: 0.4150, Time: 1710.0ms\n",
      "Iter 46570/50000, Train Loss: 3.6988, LR: 0.000002, GradNorm: 0.4129, Time: 1711.2ms\n",
      "Iter 46580/50000, Train Loss: 3.6530, LR: 0.000002, GradNorm: 0.4145, Time: 1709.8ms\n",
      "Iter 46590/50000, Train Loss: 3.6924, LR: 0.000002, GradNorm: 0.4121, Time: 1710.1ms\n",
      "Iter 46600/50000, Train Loss: 3.7253, LR: 0.000002, GradNorm: 0.4054, Time: 1709.9ms\n",
      "Iter 46610/50000, Train Loss: 3.7345, LR: 0.000002, GradNorm: 0.4110, Time: 1710.7ms\n",
      "Iter 46620/50000, Train Loss: 3.7993, LR: 0.000002, GradNorm: 0.4131, Time: 1709.2ms\n",
      "Iter 46630/50000, Train Loss: 3.7925, LR: 0.000002, GradNorm: 0.4109, Time: 1710.7ms\n",
      "Iter 46640/50000, Train Loss: 3.7282, LR: 0.000002, GradNorm: 0.4080, Time: 1710.0ms\n",
      "Iter 46650/50000, Train Loss: 3.7358, LR: 0.000002, GradNorm: 0.4187, Time: 1711.2ms\n",
      "Iter 46660/50000, Train Loss: 3.7235, LR: 0.000002, GradNorm: 0.4121, Time: 1710.9ms\n",
      "Iter 46670/50000, Train Loss: 3.7471, LR: 0.000002, GradNorm: 0.4263, Time: 1709.7ms\n",
      "Iter 46680/50000, Train Loss: 3.7232, LR: 0.000002, GradNorm: 0.4138, Time: 1710.6ms\n",
      "Iter 46690/50000, Train Loss: 3.7887, LR: 0.000002, GradNorm: 0.4093, Time: 1708.8ms\n",
      "Iter 46700/50000, Train Loss: 3.7663, LR: 0.000002, GradNorm: 0.4158, Time: 1710.4ms\n",
      "Iter 46710/50000, Train Loss: 3.7340, LR: 0.000002, GradNorm: 0.4229, Time: 1709.4ms\n",
      "Iter 46720/50000, Train Loss: 3.7607, LR: 0.000002, GradNorm: 0.4335, Time: 1710.0ms\n",
      "Iter 46730/50000, Train Loss: 3.8202, LR: 0.000002, GradNorm: 0.4289, Time: 1710.7ms\n",
      "Iter 46740/50000, Train Loss: 3.7483, LR: 0.000002, GradNorm: 0.4143, Time: 1710.8ms\n",
      "Iter 46750/50000, Train Loss: 3.7143, LR: 0.000002, GradNorm: 0.4260, Time: 1710.1ms\n",
      "Iter 46760/50000, Train Loss: 3.7818, LR: 0.000002, GradNorm: 0.4174, Time: 1710.7ms\n",
      "Iter 46770/50000, Train Loss: 3.7852, LR: 0.000002, GradNorm: 0.4142, Time: 1710.3ms\n",
      "Iter 46780/50000, Train Loss: 3.7176, LR: 0.000002, GradNorm: 0.4165, Time: 1709.9ms\n",
      "Iter 46790/50000, Train Loss: 3.7912, LR: 0.000002, GradNorm: 0.4194, Time: 1709.4ms\n",
      "Iter 46800/50000, Train Loss: 3.7295, LR: 0.000002, GradNorm: 0.4196, Time: 1710.5ms\n",
      "Iter 46810/50000, Train Loss: 3.7569, LR: 0.000002, GradNorm: 0.4037, Time: 1709.4ms\n",
      "Iter 46820/50000, Train Loss: 3.7730, LR: 0.000002, GradNorm: 0.4167, Time: 1711.8ms\n",
      "Iter 46830/50000, Train Loss: 3.7547, LR: 0.000002, GradNorm: 0.4158, Time: 1710.6ms\n",
      "Iter 46840/50000, Train Loss: 3.7507, LR: 0.000002, GradNorm: 0.4261, Time: 1710.2ms\n",
      "Iter 46850/50000, Train Loss: 3.7484, LR: 0.000002, GradNorm: 0.4167, Time: 1710.1ms\n",
      "Iter 46860/50000, Train Loss: 3.7216, LR: 0.000002, GradNorm: 0.4202, Time: 1709.3ms\n",
      "Iter 46870/50000, Train Loss: 3.7125, LR: 0.000002, GradNorm: 0.4083, Time: 1710.7ms\n",
      "Iter 46880/50000, Train Loss: 3.7762, LR: 0.000002, GradNorm: 0.4190, Time: 1709.3ms\n",
      "Iter 46890/50000, Train Loss: 3.7558, LR: 0.000002, GradNorm: 0.4105, Time: 1710.5ms\n",
      "Iter 46900/50000, Train Loss: 3.7240, LR: 0.000002, GradNorm: 0.4157, Time: 1711.2ms\n",
      "Iter 46910/50000, Train Loss: 3.7463, LR: 0.000002, GradNorm: 0.4139, Time: 1711.4ms\n",
      "Iter 46920/50000, Train Loss: 3.7749, LR: 0.000002, GradNorm: 0.4123, Time: 1710.6ms\n",
      "Iter 46930/50000, Train Loss: 3.7061, LR: 0.000002, GradNorm: 0.4088, Time: 1710.6ms\n",
      "Iter 46940/50000, Train Loss: 3.7324, LR: 0.000002, GradNorm: 0.4168, Time: 1711.3ms\n",
      "Iter 46950/50000, Train Loss: 3.7136, LR: 0.000002, GradNorm: 0.4126, Time: 1710.6ms\n",
      "Iter 46960/50000, Train Loss: 3.6606, LR: 0.000002, GradNorm: 0.4099, Time: 1710.4ms\n",
      "Iter 46970/50000, Train Loss: 3.7650, LR: 0.000002, GradNorm: 0.4179, Time: 1710.6ms\n",
      "Iter 46980/50000, Train Loss: 3.7572, LR: 0.000002, GradNorm: 0.4082, Time: 1709.9ms\n",
      "Iter 46990/50000, Train Loss: 3.7769, LR: 0.000002, GradNorm: 0.4094, Time: 1711.7ms\n",
      "Iter 47000/50000, Train Loss: 3.7665, LR: 0.000002, GradNorm: 0.4196, Time: 1710.3ms\n",
      "Iter 47010/50000, Train Loss: 3.7485, LR: 0.000002, GradNorm: 0.4213, Time: 1710.3ms\n",
      "Iter 47020/50000, Train Loss: 3.7751, LR: 0.000002, GradNorm: 0.4127, Time: 1710.2ms\n",
      "Iter 47030/50000, Train Loss: 3.6904, LR: 0.000002, GradNorm: 0.4190, Time: 1710.4ms\n",
      "Iter 47040/50000, Train Loss: 3.7426, LR: 0.000002, GradNorm: 0.4174, Time: 1711.5ms\n",
      "Iter 47050/50000, Train Loss: 3.7677, LR: 0.000002, GradNorm: 0.4134, Time: 1710.1ms\n",
      "Iter 47060/50000, Train Loss: 3.7741, LR: 0.000002, GradNorm: 0.4161, Time: 1710.6ms\n",
      "Iter 47070/50000, Train Loss: 3.7283, LR: 0.000002, GradNorm: 0.4083, Time: 1711.0ms\n",
      "Iter 47080/50000, Train Loss: 3.7664, LR: 0.000002, GradNorm: 0.4117, Time: 1711.5ms\n",
      "Iter 47090/50000, Train Loss: 3.7177, LR: 0.000002, GradNorm: 0.4131, Time: 1710.1ms\n",
      "Iter 47100/50000, Train Loss: 3.7019, LR: 0.000002, GradNorm: 0.4090, Time: 1710.9ms\n",
      "Iter 47110/50000, Train Loss: 3.7657, LR: 0.000002, GradNorm: 0.4161, Time: 1709.9ms\n",
      "Iter 47120/50000, Train Loss: 3.7186, LR: 0.000002, GradNorm: 0.4249, Time: 1708.2ms\n",
      "Iter 47130/50000, Train Loss: 3.7640, LR: 0.000002, GradNorm: 0.4109, Time: 1709.3ms\n",
      "Iter 47140/50000, Train Loss: 3.7515, LR: 0.000002, GradNorm: 0.4217, Time: 1708.6ms\n",
      "Iter 47150/50000, Train Loss: 3.7092, LR: 0.000002, GradNorm: 0.4202, Time: 1707.6ms\n",
      "Iter 47160/50000, Train Loss: 3.7620, LR: 0.000002, GradNorm: 0.4188, Time: 1708.9ms\n",
      "Iter 47170/50000, Train Loss: 3.7022, LR: 0.000002, GradNorm: 0.4181, Time: 1708.4ms\n",
      "Iter 47180/50000, Train Loss: 3.7423, LR: 0.000002, GradNorm: 0.4110, Time: 1708.0ms\n",
      "Iter 47190/50000, Train Loss: 3.7506, LR: 0.000002, GradNorm: 0.4078, Time: 1708.1ms\n",
      "Iter 47200/50000, Train Loss: 3.7571, LR: 0.000002, GradNorm: 0.4144, Time: 1708.4ms\n",
      "Iter 47210/50000, Train Loss: 3.7314, LR: 0.000002, GradNorm: 0.4183, Time: 1709.3ms\n",
      "Iter 47220/50000, Train Loss: 3.7280, LR: 0.000002, GradNorm: 0.4103, Time: 1708.0ms\n",
      "Iter 47230/50000, Train Loss: 3.7963, LR: 0.000002, GradNorm: 0.4204, Time: 1707.8ms\n",
      "Iter 47240/50000, Train Loss: 3.7436, LR: 0.000002, GradNorm: 0.4143, Time: 1708.3ms\n",
      "Iter 47250/50000, Train Loss: 3.7442, LR: 0.000002, GradNorm: 0.4134, Time: 1709.3ms\n",
      "Iter 47260/50000, Train Loss: 3.7230, LR: 0.000002, GradNorm: 0.4110, Time: 1708.5ms\n",
      "Iter 47270/50000, Train Loss: 3.7440, LR: 0.000002, GradNorm: 0.4157, Time: 1708.1ms\n",
      "Iter 47280/50000, Train Loss: 3.7907, LR: 0.000002, GradNorm: 0.4249, Time: 1708.1ms\n",
      "Iter 47290/50000, Train Loss: 3.7653, LR: 0.000002, GradNorm: 0.4131, Time: 1709.0ms\n",
      "Iter 47300/50000, Train Loss: 3.7683, LR: 0.000002, GradNorm: 0.4166, Time: 1707.7ms\n",
      "Iter 47310/50000, Train Loss: 3.7185, LR: 0.000002, GradNorm: 0.4077, Time: 1709.7ms\n",
      "Iter 47320/50000, Train Loss: 3.7457, LR: 0.000002, GradNorm: 0.4085, Time: 1708.3ms\n",
      "Iter 47330/50000, Train Loss: 3.7230, LR: 0.000002, GradNorm: 0.4121, Time: 1709.1ms\n",
      "Iter 47340/50000, Train Loss: 3.7592, LR: 0.000002, GradNorm: 0.4133, Time: 1708.3ms\n",
      "Iter 47350/50000, Train Loss: 3.7556, LR: 0.000002, GradNorm: 0.4764, Time: 1708.6ms\n",
      "Iter 47360/50000, Train Loss: 3.7943, LR: 0.000002, GradNorm: 0.4169, Time: 1708.1ms\n",
      "Iter 47370/50000, Train Loss: 3.7492, LR: 0.000002, GradNorm: 0.4150, Time: 1707.6ms\n",
      "Iter 47380/50000, Train Loss: 3.6739, LR: 0.000002, GradNorm: 0.4127, Time: 1709.9ms\n",
      "Iter 47390/50000, Train Loss: 3.7381, LR: 0.000002, GradNorm: 0.4130, Time: 1708.1ms\n",
      "Iter 47400/50000, Train Loss: 3.7347, LR: 0.000002, GradNorm: 0.4207, Time: 1708.2ms\n",
      "Iter 47410/50000, Train Loss: 3.7619, LR: 0.000002, GradNorm: 0.4141, Time: 1708.0ms\n",
      "Iter 47420/50000, Train Loss: 3.6645, LR: 0.000002, GradNorm: 0.4241, Time: 1709.9ms\n",
      "Iter 47430/50000, Train Loss: 3.7705, LR: 0.000002, GradNorm: 0.4142, Time: 1708.3ms\n",
      "Iter 47440/50000, Train Loss: 3.7220, LR: 0.000002, GradNorm: 0.4218, Time: 1708.2ms\n",
      "Iter 47450/50000, Train Loss: 3.7799, LR: 0.000002, GradNorm: 0.4076, Time: 1709.0ms\n",
      "Iter 47460/50000, Train Loss: 3.6949, LR: 0.000002, GradNorm: 0.4144, Time: 1709.2ms\n",
      "Iter 47470/50000, Train Loss: 3.7701, LR: 0.000002, GradNorm: 0.4181, Time: 1708.7ms\n",
      "Iter 47480/50000, Train Loss: 3.7598, LR: 0.000002, GradNorm: 0.4173, Time: 1708.6ms\n",
      "Iter 47490/50000, Train Loss: 3.7433, LR: 0.000002, GradNorm: 0.4172, Time: 1708.2ms\n",
      "Iter 47500/50000, Train Loss: 3.7517, LR: 0.000002, GradNorm: 0.4222, Time: 1709.9ms\n",
      "Iter 47510/50000, Train Loss: 3.7578, LR: 0.000002, GradNorm: 0.4101, Time: 1708.8ms\n",
      "Iter 47520/50000, Train Loss: 3.7344, LR: 0.000002, GradNorm: 0.4122, Time: 1708.9ms\n",
      "Iter 47530/50000, Train Loss: 3.7025, LR: 0.000002, GradNorm: 0.4152, Time: 1708.9ms\n",
      "Iter 47540/50000, Train Loss: 3.7593, LR: 0.000002, GradNorm: 0.4172, Time: 1708.7ms\n",
      "Iter 47550/50000, Train Loss: 3.7709, LR: 0.000002, GradNorm: 0.4235, Time: 1708.6ms\n",
      "Iter 47560/50000, Train Loss: 3.7224, LR: 0.000002, GradNorm: 0.4165, Time: 1709.2ms\n",
      "Iter 47570/50000, Train Loss: 3.7043, LR: 0.000002, GradNorm: 0.4178, Time: 1707.4ms\n",
      "Iter 47580/50000, Train Loss: 3.7739, LR: 0.000002, GradNorm: 0.4400, Time: 1708.3ms\n",
      "Iter 47590/50000, Train Loss: 3.6837, LR: 0.000002, GradNorm: 0.4119, Time: 1709.5ms\n",
      "Iter 47600/50000, Train Loss: 3.7587, LR: 0.000002, GradNorm: 0.4181, Time: 1709.2ms\n",
      "Iter 47610/50000, Train Loss: 3.7349, LR: 0.000002, GradNorm: 0.4077, Time: 1707.6ms\n",
      "Iter 47620/50000, Train Loss: 3.7452, LR: 0.000002, GradNorm: 0.4050, Time: 1707.5ms\n",
      "Iter 47630/50000, Train Loss: 3.7300, LR: 0.000002, GradNorm: 0.4153, Time: 1708.0ms\n",
      "Iter 47640/50000, Train Loss: 3.7844, LR: 0.000002, GradNorm: 0.4197, Time: 1708.3ms\n",
      "Iter 47650/50000, Train Loss: 3.7155, LR: 0.000002, GradNorm: 0.4112, Time: 1708.3ms\n",
      "Iter 47660/50000, Train Loss: 3.7644, LR: 0.000002, GradNorm: 0.4201, Time: 1708.2ms\n",
      "Iter 47670/50000, Train Loss: 3.7579, LR: 0.000002, GradNorm: 0.4134, Time: 1708.8ms\n",
      "Iter 47680/50000, Train Loss: 3.7044, LR: 0.000002, GradNorm: 0.4112, Time: 1709.2ms\n",
      "Iter 47690/50000, Train Loss: 3.7380, LR: 0.000002, GradNorm: 0.4139, Time: 1708.8ms\n",
      "Iter 47700/50000, Train Loss: 3.7317, LR: 0.000002, GradNorm: 0.4100, Time: 1708.7ms\n",
      "Iter 47710/50000, Train Loss: 3.7703, LR: 0.000002, GradNorm: 0.4143, Time: 1707.3ms\n",
      "Iter 47720/50000, Train Loss: 3.7893, LR: 0.000002, GradNorm: 0.4137, Time: 1708.8ms\n",
      "Iter 47730/50000, Train Loss: 3.7605, LR: 0.000002, GradNorm: 0.4114, Time: 1707.9ms\n",
      "Iter 47740/50000, Train Loss: 3.8130, LR: 0.000002, GradNorm: 0.4123, Time: 1708.7ms\n",
      "Iter 47750/50000, Train Loss: 3.7571, LR: 0.000002, GradNorm: 0.4204, Time: 1708.6ms\n",
      "Iter 47760/50000, Train Loss: 3.6927, LR: 0.000002, GradNorm: 0.4130, Time: 1709.1ms\n",
      "Iter 47770/50000, Train Loss: 3.7477, LR: 0.000002, GradNorm: 0.4124, Time: 1709.0ms\n",
      "Iter 47780/50000, Train Loss: 3.7464, LR: 0.000002, GradNorm: 0.4119, Time: 1709.1ms\n",
      "Iter 47790/50000, Train Loss: 3.7390, LR: 0.000002, GradNorm: 0.4157, Time: 1708.0ms\n",
      "Iter 47800/50000, Train Loss: 3.7987, LR: 0.000002, GradNorm: 0.4204, Time: 1708.2ms\n",
      "Iter 47810/50000, Train Loss: 3.7344, LR: 0.000002, GradNorm: 0.4134, Time: 1709.6ms\n",
      "Iter 47820/50000, Train Loss: 3.7912, LR: 0.000002, GradNorm: 0.4155, Time: 1708.5ms\n",
      "Iter 47830/50000, Train Loss: 3.8062, LR: 0.000002, GradNorm: 0.4191, Time: 1707.7ms\n",
      "Iter 47840/50000, Train Loss: 3.7326, LR: 0.000002, GradNorm: 0.4117, Time: 1708.1ms\n",
      "Iter 47850/50000, Train Loss: 3.7704, LR: 0.000002, GradNorm: 0.4127, Time: 1709.0ms\n",
      "Iter 47860/50000, Train Loss: 3.8102, LR: 0.000002, GradNorm: 0.4104, Time: 1708.5ms\n",
      "Iter 47870/50000, Train Loss: 3.7665, LR: 0.000002, GradNorm: 0.4221, Time: 1707.7ms\n",
      "Iter 47880/50000, Train Loss: 3.6947, LR: 0.000002, GradNorm: 0.4154, Time: 1708.5ms\n",
      "Iter 47890/50000, Train Loss: 3.6943, LR: 0.000002, GradNorm: 0.4108, Time: 1708.9ms\n",
      "Iter 47900/50000, Train Loss: 3.7112, LR: 0.000002, GradNorm: 0.4174, Time: 1709.2ms\n",
      "Iter 47910/50000, Train Loss: 3.7557, LR: 0.000002, GradNorm: 0.4115, Time: 1707.4ms\n",
      "Iter 47920/50000, Train Loss: 3.7312, LR: 0.000002, GradNorm: 0.4090, Time: 1707.7ms\n",
      "Iter 47930/50000, Train Loss: 3.7058, LR: 0.000002, GradNorm: 0.4145, Time: 1709.3ms\n",
      "Iter 47940/50000, Train Loss: 3.6762, LR: 0.000002, GradNorm: 0.4103, Time: 1708.0ms\n",
      "Iter 47950/50000, Train Loss: 3.7912, LR: 0.000002, GradNorm: 0.4166, Time: 1708.3ms\n",
      "Iter 47960/50000, Train Loss: 3.7406, LR: 0.000002, GradNorm: 0.4092, Time: 1708.9ms\n",
      "Iter 47970/50000, Train Loss: 3.7381, LR: 0.000002, GradNorm: 0.4203, Time: 1709.7ms\n",
      "Iter 47980/50000, Train Loss: 3.7409, LR: 0.000002, GradNorm: 0.4211, Time: 1708.8ms\n",
      "Iter 47990/50000, Train Loss: 3.8109, LR: 0.000002, GradNorm: 0.4161, Time: 1707.6ms\n",
      "Iter 48000/50000, Train Loss: 3.7121, LR: 0.000002, GradNorm: 0.4112, Time: 1708.8ms\n",
      "Iter 48010/50000, Train Loss: 3.7551, LR: 0.000002, GradNorm: 0.4179, Time: 1709.4ms\n",
      "Iter 48020/50000, Train Loss: 3.7550, LR: 0.000002, GradNorm: 0.4236, Time: 1710.1ms\n",
      "Iter 48030/50000, Train Loss: 3.7553, LR: 0.000002, GradNorm: 0.4175, Time: 1707.6ms\n",
      "Iter 48040/50000, Train Loss: 3.7454, LR: 0.000002, GradNorm: 0.4108, Time: 1707.5ms\n",
      "Iter 48050/50000, Train Loss: 3.6765, LR: 0.000002, GradNorm: 0.4193, Time: 1708.9ms\n",
      "Iter 48060/50000, Train Loss: 3.7468, LR: 0.000002, GradNorm: 0.4111, Time: 1708.9ms\n",
      "Iter 48070/50000, Train Loss: 3.7071, LR: 0.000002, GradNorm: 0.4173, Time: 1708.4ms\n",
      "Iter 48080/50000, Train Loss: 3.7812, LR: 0.000002, GradNorm: 0.4128, Time: 1708.8ms\n",
      "Iter 48090/50000, Train Loss: 3.7602, LR: 0.000002, GradNorm: 0.4142, Time: 1710.8ms\n",
      "Iter 48100/50000, Train Loss: 3.7633, LR: 0.000002, GradNorm: 0.4177, Time: 1711.8ms\n",
      "Iter 48110/50000, Train Loss: 3.6966, LR: 0.000002, GradNorm: 0.4144, Time: 1710.3ms\n",
      "Iter 48120/50000, Train Loss: 3.7089, LR: 0.000002, GradNorm: 0.4192, Time: 1710.3ms\n",
      "Iter 48130/50000, Train Loss: 3.8066, LR: 0.000002, GradNorm: 0.4094, Time: 1710.4ms\n",
      "Iter 48140/50000, Train Loss: 3.7770, LR: 0.000002, GradNorm: 0.4055, Time: 1709.9ms\n",
      "Iter 48150/50000, Train Loss: 3.7444, LR: 0.000002, GradNorm: 0.4289, Time: 1710.5ms\n",
      "Iter 48160/50000, Train Loss: 3.7700, LR: 0.000002, GradNorm: 0.4164, Time: 1710.8ms\n",
      "Iter 48170/50000, Train Loss: 3.7206, LR: 0.000002, GradNorm: 0.4087, Time: 1709.7ms\n",
      "Iter 48180/50000, Train Loss: 3.7353, LR: 0.000002, GradNorm: 0.4210, Time: 1710.5ms\n",
      "Iter 48190/50000, Train Loss: 3.7531, LR: 0.000002, GradNorm: 0.4114, Time: 1710.9ms\n",
      "Iter 48200/50000, Train Loss: 3.7659, LR: 0.000002, GradNorm: 0.4047, Time: 1710.5ms\n",
      "Iter 48210/50000, Train Loss: 3.7797, LR: 0.000002, GradNorm: 0.4179, Time: 1711.2ms\n",
      "Iter 48220/50000, Train Loss: 3.6978, LR: 0.000002, GradNorm: 0.4140, Time: 1711.1ms\n",
      "Iter 48230/50000, Train Loss: 3.6699, LR: 0.000002, GradNorm: 0.4533, Time: 1710.6ms\n",
      "Iter 48240/50000, Train Loss: 3.7712, LR: 0.000002, GradNorm: 0.4152, Time: 1710.1ms\n",
      "Iter 48250/50000, Train Loss: 3.7025, LR: 0.000002, GradNorm: 0.4163, Time: 1711.3ms\n",
      "Iter 48260/50000, Train Loss: 3.7341, LR: 0.000002, GradNorm: 0.4130, Time: 1709.9ms\n",
      "Iter 48270/50000, Train Loss: 3.7828, LR: 0.000002, GradNorm: 0.4151, Time: 1711.2ms\n",
      "Iter 48280/50000, Train Loss: 3.7145, LR: 0.000002, GradNorm: 0.4149, Time: 1710.7ms\n",
      "Iter 48290/50000, Train Loss: 3.7604, LR: 0.000002, GradNorm: 0.4097, Time: 1710.6ms\n",
      "Iter 48300/50000, Train Loss: 3.6563, LR: 0.000002, GradNorm: 0.4115, Time: 1709.9ms\n",
      "Iter 48310/50000, Train Loss: 3.7858, LR: 0.000002, GradNorm: 0.4154, Time: 1710.0ms\n",
      "Iter 48320/50000, Train Loss: 3.7403, LR: 0.000002, GradNorm: 0.4123, Time: 1711.5ms\n",
      "Iter 48330/50000, Train Loss: 3.7426, LR: 0.000002, GradNorm: 0.4157, Time: 1710.0ms\n",
      "Iter 48340/50000, Train Loss: 3.7829, LR: 0.000002, GradNorm: 0.4162, Time: 1710.3ms\n",
      "Iter 48350/50000, Train Loss: 3.7763, LR: 0.000002, GradNorm: 0.4071, Time: 1710.0ms\n",
      "Iter 48360/50000, Train Loss: 3.7402, LR: 0.000002, GradNorm: 0.4267, Time: 1711.7ms\n",
      "Iter 48370/50000, Train Loss: 3.7626, LR: 0.000002, GradNorm: 0.4112, Time: 1709.8ms\n",
      "Iter 48380/50000, Train Loss: 3.6965, LR: 0.000002, GradNorm: 0.4168, Time: 1710.6ms\n",
      "Iter 48390/50000, Train Loss: 3.7823, LR: 0.000002, GradNorm: 0.4078, Time: 1710.6ms\n",
      "Iter 48400/50000, Train Loss: 3.7909, LR: 0.000002, GradNorm: 0.4128, Time: 1711.5ms\n",
      "Iter 48410/50000, Train Loss: 3.7706, LR: 0.000002, GradNorm: 0.4245, Time: 1709.7ms\n",
      "Iter 48420/50000, Train Loss: 3.7555, LR: 0.000002, GradNorm: 0.4123, Time: 1709.8ms\n",
      "Iter 48430/50000, Train Loss: 3.7464, LR: 0.000002, GradNorm: 0.4256, Time: 1710.6ms\n",
      "Iter 48440/50000, Train Loss: 3.7615, LR: 0.000002, GradNorm: 0.4125, Time: 1711.6ms\n",
      "Iter 48450/50000, Train Loss: 3.7527, LR: 0.000002, GradNorm: 0.4157, Time: 1710.8ms\n",
      "Iter 48460/50000, Train Loss: 3.7199, LR: 0.000002, GradNorm: 0.4096, Time: 1711.2ms\n",
      "Iter 48470/50000, Train Loss: 3.7429, LR: 0.000002, GradNorm: 0.4197, Time: 1711.6ms\n",
      "Iter 48480/50000, Train Loss: 3.7755, LR: 0.000002, GradNorm: 0.4183, Time: 1709.8ms\n",
      "Iter 48490/50000, Train Loss: 3.6832, LR: 0.000002, GradNorm: 0.4172, Time: 1709.6ms\n",
      "Iter 48500/50000, Train Loss: 3.7229, LR: 0.000002, GradNorm: 0.4428, Time: 1709.5ms\n",
      "Iter 48510/50000, Train Loss: 3.7399, LR: 0.000002, GradNorm: 0.4255, Time: 1709.5ms\n",
      "Iter 48520/50000, Train Loss: 3.8121, LR: 0.000002, GradNorm: 0.4110, Time: 1710.3ms\n",
      "Iter 48530/50000, Train Loss: 3.7521, LR: 0.000002, GradNorm: 0.4172, Time: 1711.0ms\n",
      "Iter 48540/50000, Train Loss: 3.7735, LR: 0.000002, GradNorm: 0.4173, Time: 1710.5ms\n",
      "Iter 48550/50000, Train Loss: 3.7051, LR: 0.000002, GradNorm: 0.4083, Time: 1710.0ms\n",
      "Iter 48560/50000, Train Loss: 3.7394, LR: 0.000002, GradNorm: 0.4133, Time: 1710.2ms\n",
      "Iter 48570/50000, Train Loss: 3.7411, LR: 0.000002, GradNorm: 0.4158, Time: 1709.7ms\n",
      "Iter 48580/50000, Train Loss: 3.7039, LR: 0.000002, GradNorm: 0.4189, Time: 1709.3ms\n",
      "Iter 48590/50000, Train Loss: 3.7617, LR: 0.000002, GradNorm: 0.4098, Time: 1709.5ms\n",
      "Iter 48600/50000, Train Loss: 3.7772, LR: 0.000002, GradNorm: 0.4233, Time: 1710.3ms\n",
      "Iter 48610/50000, Train Loss: 3.7166, LR: 0.000002, GradNorm: 0.4123, Time: 1711.5ms\n",
      "Iter 48620/50000, Train Loss: 3.7744, LR: 0.000002, GradNorm: 0.4138, Time: 1710.4ms\n",
      "Iter 48630/50000, Train Loss: 3.7870, LR: 0.000002, GradNorm: 0.4292, Time: 1710.6ms\n",
      "Iter 48640/50000, Train Loss: 3.7724, LR: 0.000002, GradNorm: 0.4235, Time: 1710.2ms\n",
      "Iter 48650/50000, Train Loss: 3.6863, LR: 0.000002, GradNorm: 0.4123, Time: 1709.8ms\n",
      "Iter 48660/50000, Train Loss: 3.6975, LR: 0.000002, GradNorm: 0.4128, Time: 1710.1ms\n",
      "Iter 48670/50000, Train Loss: 3.7000, LR: 0.000002, GradNorm: 0.4168, Time: 1710.9ms\n",
      "Iter 48680/50000, Train Loss: 3.7259, LR: 0.000002, GradNorm: 0.4141, Time: 1711.2ms\n",
      "Iter 48690/50000, Train Loss: 3.7296, LR: 0.000002, GradNorm: 0.4190, Time: 1711.0ms\n",
      "Iter 48700/50000, Train Loss: 3.6669, LR: 0.000002, GradNorm: 0.4164, Time: 1711.9ms\n",
      "Iter 48710/50000, Train Loss: 3.7177, LR: 0.000002, GradNorm: 0.4179, Time: 1710.0ms\n",
      "Iter 48720/50000, Train Loss: 3.7152, LR: 0.000002, GradNorm: 0.4118, Time: 1710.2ms\n",
      "Iter 48730/50000, Train Loss: 3.7606, LR: 0.000002, GradNorm: 0.4125, Time: 1710.7ms\n",
      "Iter 48740/50000, Train Loss: 3.7017, LR: 0.000002, GradNorm: 0.4151, Time: 1710.7ms\n",
      "Iter 48750/50000, Train Loss: 3.7507, LR: 0.000002, GradNorm: 0.4095, Time: 1710.4ms\n",
      "Iter 48760/50000, Train Loss: 3.7676, LR: 0.000002, GradNorm: 0.4157, Time: 1710.0ms\n",
      "Iter 48770/50000, Train Loss: 3.7245, LR: 0.000002, GradNorm: 0.4151, Time: 1710.3ms\n",
      "Iter 48780/50000, Train Loss: 3.7595, LR: 0.000002, GradNorm: 0.4107, Time: 1711.7ms\n",
      "Iter 48790/50000, Train Loss: 3.7128, LR: 0.000002, GradNorm: 0.4140, Time: 1710.8ms\n",
      "Iter 48800/50000, Train Loss: 3.7928, LR: 0.000002, GradNorm: 0.4131, Time: 1710.4ms\n",
      "Iter 48810/50000, Train Loss: 3.7585, LR: 0.000002, GradNorm: 0.4209, Time: 1710.6ms\n",
      "Iter 48820/50000, Train Loss: 3.7642, LR: 0.000002, GradNorm: 0.4151, Time: 1710.4ms\n",
      "Iter 48830/50000, Train Loss: 3.7192, LR: 0.000002, GradNorm: 0.4129, Time: 1710.3ms\n",
      "Iter 48840/50000, Train Loss: 3.7260, LR: 0.000002, GradNorm: 0.4106, Time: 1710.5ms\n",
      "Iter 48850/50000, Train Loss: 3.7819, LR: 0.000002, GradNorm: 0.4181, Time: 1710.3ms\n",
      "Iter 48860/50000, Train Loss: 3.7046, LR: 0.000002, GradNorm: 0.4164, Time: 1710.2ms\n",
      "Iter 48870/50000, Train Loss: 3.7807, LR: 0.000002, GradNorm: 0.4191, Time: 1711.6ms\n",
      "Iter 48880/50000, Train Loss: 3.7342, LR: 0.000002, GradNorm: 0.4282, Time: 1711.6ms\n",
      "Iter 48890/50000, Train Loss: 3.7035, LR: 0.000002, GradNorm: 0.4161, Time: 1710.0ms\n",
      "Iter 48900/50000, Train Loss: 3.7241, LR: 0.000002, GradNorm: 0.4066, Time: 1711.1ms\n",
      "Iter 48910/50000, Train Loss: 3.7918, LR: 0.000002, GradNorm: 0.4244, Time: 1711.0ms\n",
      "Iter 48920/50000, Train Loss: 3.7775, LR: 0.000002, GradNorm: 0.4177, Time: 1709.7ms\n",
      "Iter 48930/50000, Train Loss: 3.7615, LR: 0.000002, GradNorm: 0.4151, Time: 1710.1ms\n",
      "Iter 48940/50000, Train Loss: 3.6870, LR: 0.000002, GradNorm: 0.4102, Time: 1710.9ms\n",
      "Iter 48950/50000, Train Loss: 3.7261, LR: 0.000002, GradNorm: 0.4261, Time: 1711.0ms\n",
      "Iter 48960/50000, Train Loss: 3.7762, LR: 0.000002, GradNorm: 0.4174, Time: 1709.9ms\n",
      "Iter 48970/50000, Train Loss: 3.8071, LR: 0.000002, GradNorm: 0.4283, Time: 1710.7ms\n",
      "Iter 48980/50000, Train Loss: 3.7233, LR: 0.000002, GradNorm: 0.4210, Time: 1711.1ms\n",
      "Iter 48990/50000, Train Loss: 3.7648, LR: 0.000002, GradNorm: 0.4141, Time: 1709.9ms\n",
      "Iter 49000/50000, Train Loss: 3.7986, LR: 0.000002, GradNorm: 0.4154, Time: 1711.2ms\n",
      "Iter 49010/50000, Train Loss: 3.7222, LR: 0.000002, GradNorm: 0.4140, Time: 1710.5ms\n",
      "Iter 49020/50000, Train Loss: 3.7048, LR: 0.000002, GradNorm: 0.4350, Time: 1709.7ms\n",
      "Iter 49030/50000, Train Loss: 3.7195, LR: 0.000002, GradNorm: 0.4103, Time: 1710.8ms\n",
      "Iter 49040/50000, Train Loss: 3.6486, LR: 0.000002, GradNorm: 0.4111, Time: 1712.4ms\n",
      "Iter 49050/50000, Train Loss: 3.7525, LR: 0.000002, GradNorm: 0.4116, Time: 1710.6ms\n",
      "Iter 49060/50000, Train Loss: 3.7400, LR: 0.000002, GradNorm: 0.4204, Time: 1710.8ms\n",
      "Iter 49070/50000, Train Loss: 3.8057, LR: 0.000002, GradNorm: 0.4175, Time: 1710.9ms\n",
      "Iter 49080/50000, Train Loss: 3.7827, LR: 0.000002, GradNorm: 0.4215, Time: 1711.0ms\n",
      "Iter 49090/50000, Train Loss: 3.7856, LR: 0.000002, GradNorm: 0.4178, Time: 1710.1ms\n",
      "Iter 49100/50000, Train Loss: 3.7094, LR: 0.000002, GradNorm: 0.4202, Time: 1710.2ms\n",
      "Iter 49110/50000, Train Loss: 3.8269, LR: 0.000002, GradNorm: 0.4123, Time: 1710.1ms\n",
      "Iter 49120/50000, Train Loss: 3.6910, LR: 0.000002, GradNorm: 0.4055, Time: 1711.8ms\n",
      "Iter 49130/50000, Train Loss: 3.7200, LR: 0.000002, GradNorm: 0.4200, Time: 1710.9ms\n",
      "Iter 49140/50000, Train Loss: 3.8107, LR: 0.000002, GradNorm: 0.4146, Time: 1710.5ms\n",
      "Iter 49150/50000, Train Loss: 3.7103, LR: 0.000002, GradNorm: 0.4086, Time: 1711.0ms\n",
      "Iter 49160/50000, Train Loss: 3.7233, LR: 0.000002, GradNorm: 0.4153, Time: 1710.1ms\n",
      "Iter 49170/50000, Train Loss: 3.7782, LR: 0.000002, GradNorm: 0.4167, Time: 1710.0ms\n",
      "Iter 49180/50000, Train Loss: 3.7539, LR: 0.000002, GradNorm: 0.4106, Time: 1709.8ms\n",
      "Iter 49190/50000, Train Loss: 3.7284, LR: 0.000002, GradNorm: 0.4192, Time: 1710.0ms\n",
      "Iter 49200/50000, Train Loss: 3.7507, LR: 0.000002, GradNorm: 0.4143, Time: 1710.1ms\n",
      "Iter 49210/50000, Train Loss: 3.7355, LR: 0.000002, GradNorm: 0.4211, Time: 1712.0ms\n",
      "Iter 49220/50000, Train Loss: 3.7770, LR: 0.000002, GradNorm: 0.4170, Time: 1710.6ms\n",
      "Iter 49230/50000, Train Loss: 3.7033, LR: 0.000002, GradNorm: 0.4137, Time: 1710.8ms\n",
      "Iter 49240/50000, Train Loss: 3.7973, LR: 0.000002, GradNorm: 0.4137, Time: 1711.3ms\n",
      "Iter 49250/50000, Train Loss: 3.7201, LR: 0.000002, GradNorm: 0.4116, Time: 1711.3ms\n",
      "Iter 49260/50000, Train Loss: 3.7252, LR: 0.000002, GradNorm: 0.4115, Time: 1710.8ms\n",
      "Iter 49270/50000, Train Loss: 3.6957, LR: 0.000002, GradNorm: 0.4138, Time: 1708.9ms\n",
      "Iter 49280/50000, Train Loss: 3.7651, LR: 0.000002, GradNorm: 0.4183, Time: 1711.3ms\n",
      "Iter 49290/50000, Train Loss: 3.7284, LR: 0.000002, GradNorm: 0.4224, Time: 1712.0ms\n",
      "Iter 49300/50000, Train Loss: 3.7817, LR: 0.000002, GradNorm: 0.4140, Time: 1712.0ms\n",
      "Iter 49310/50000, Train Loss: 3.6710, LR: 0.000002, GradNorm: 0.4222, Time: 1711.0ms\n",
      "Iter 49320/50000, Train Loss: 3.7874, LR: 0.000002, GradNorm: 0.4215, Time: 1710.8ms\n",
      "Iter 49330/50000, Train Loss: 3.7768, LR: 0.000002, GradNorm: 0.4122, Time: 1710.3ms\n",
      "Iter 49340/50000, Train Loss: 3.7609, LR: 0.000002, GradNorm: 0.4320, Time: 1711.2ms\n",
      "Iter 49350/50000, Train Loss: 3.7679, LR: 0.000002, GradNorm: 0.4263, Time: 1711.0ms\n",
      "Iter 49360/50000, Train Loss: 3.7028, LR: 0.000002, GradNorm: 0.4227, Time: 1710.1ms\n",
      "Iter 49370/50000, Train Loss: 3.7869, LR: 0.000002, GradNorm: 0.4214, Time: 1711.4ms\n",
      "Iter 49380/50000, Train Loss: 3.7042, LR: 0.000002, GradNorm: 0.4271, Time: 1712.8ms\n",
      "Iter 49390/50000, Train Loss: 3.7625, LR: 0.000002, GradNorm: 0.4119, Time: 1710.4ms\n",
      "Iter 49400/50000, Train Loss: 3.7016, LR: 0.000002, GradNorm: 0.4163, Time: 1710.9ms\n",
      "Iter 49410/50000, Train Loss: 3.7025, LR: 0.000002, GradNorm: 0.4156, Time: 1710.8ms\n",
      "Iter 49420/50000, Train Loss: 3.7651, LR: 0.000002, GradNorm: 0.4154, Time: 1712.0ms\n",
      "Iter 49430/50000, Train Loss: 3.6869, LR: 0.000002, GradNorm: 0.4118, Time: 1711.2ms\n",
      "Iter 49440/50000, Train Loss: 3.7790, LR: 0.000002, GradNorm: 0.4116, Time: 1711.5ms\n",
      "Iter 49450/50000, Train Loss: 3.7519, LR: 0.000002, GradNorm: 0.4309, Time: 1711.5ms\n",
      "Iter 49460/50000, Train Loss: 3.7825, LR: 0.000002, GradNorm: 0.4091, Time: 1713.1ms\n",
      "Iter 49470/50000, Train Loss: 3.8148, LR: 0.000002, GradNorm: 0.4202, Time: 1711.3ms\n",
      "Iter 49480/50000, Train Loss: 3.7720, LR: 0.000002, GradNorm: 0.4194, Time: 1711.1ms\n",
      "Iter 49490/50000, Train Loss: 3.7818, LR: 0.000002, GradNorm: 0.4150, Time: 1711.3ms\n",
      "Iter 49500/50000, Train Loss: 3.7425, LR: 0.000002, GradNorm: 0.4279, Time: 1710.6ms\n",
      "Iter 49510/50000, Train Loss: 3.7636, LR: 0.000002, GradNorm: 0.4161, Time: 1711.3ms\n",
      "Iter 49520/50000, Train Loss: 3.6820, LR: 0.000002, GradNorm: 0.4147, Time: 1711.7ms\n",
      "Iter 49530/50000, Train Loss: 3.7954, LR: 0.000002, GradNorm: 0.4196, Time: 1711.0ms\n",
      "Iter 49540/50000, Train Loss: 3.6643, LR: 0.000002, GradNorm: 0.4172, Time: 1711.0ms\n",
      "Iter 49550/50000, Train Loss: 3.6869, LR: 0.000002, GradNorm: 0.4186, Time: 1711.9ms\n",
      "Iter 49560/50000, Train Loss: 3.7408, LR: 0.000002, GradNorm: 0.4192, Time: 1711.8ms\n",
      "Iter 49570/50000, Train Loss: 3.7195, LR: 0.000002, GradNorm: 0.4169, Time: 1710.9ms\n",
      "Iter 49580/50000, Train Loss: 3.7579, LR: 0.000002, GradNorm: 0.4105, Time: 1711.6ms\n",
      "Iter 49590/50000, Train Loss: 3.7059, LR: 0.000002, GradNorm: 0.4167, Time: 1711.1ms\n",
      "Iter 49600/50000, Train Loss: 3.6675, LR: 0.000002, GradNorm: 0.4163, Time: 1710.8ms\n",
      "Iter 49610/50000, Train Loss: 3.7515, LR: 0.000002, GradNorm: 0.4217, Time: 1710.9ms\n",
      "Iter 49620/50000, Train Loss: 3.7282, LR: 0.000002, GradNorm: 0.4214, Time: 1711.8ms\n",
      "Iter 49630/50000, Train Loss: 3.7652, LR: 0.000002, GradNorm: 0.4203, Time: 1713.9ms\n",
      "Iter 49640/50000, Train Loss: 3.6886, LR: 0.000002, GradNorm: 0.4157, Time: 1710.9ms\n",
      "Iter 49650/50000, Train Loss: 3.7163, LR: 0.000002, GradNorm: 0.4104, Time: 1711.7ms\n",
      "Iter 49660/50000, Train Loss: 3.7585, LR: 0.000002, GradNorm: 0.4196, Time: 1711.5ms\n",
      "Iter 49670/50000, Train Loss: 3.7943, LR: 0.000002, GradNorm: 0.4177, Time: 1711.3ms\n",
      "Iter 49680/50000, Train Loss: 3.7295, LR: 0.000002, GradNorm: 0.4169, Time: 1711.2ms\n",
      "Iter 49690/50000, Train Loss: 3.7447, LR: 0.000002, GradNorm: 0.4135, Time: 1711.0ms\n",
      "Iter 49700/50000, Train Loss: 3.7182, LR: 0.000002, GradNorm: 0.4045, Time: 1712.1ms\n",
      "Iter 49710/50000, Train Loss: 3.7186, LR: 0.000002, GradNorm: 0.4337, Time: 1712.5ms\n",
      "Iter 49720/50000, Train Loss: 3.7582, LR: 0.000002, GradNorm: 0.4183, Time: 1712.1ms\n",
      "Iter 49730/50000, Train Loss: 3.7371, LR: 0.000002, GradNorm: 0.4222, Time: 1712.4ms\n",
      "Iter 49740/50000, Train Loss: 3.7016, LR: 0.000002, GradNorm: 0.4252, Time: 1711.2ms\n",
      "Iter 49750/50000, Train Loss: 3.7684, LR: 0.000002, GradNorm: 0.4156, Time: 1711.2ms\n",
      "Iter 49760/50000, Train Loss: 3.7839, LR: 0.000002, GradNorm: 0.4185, Time: 1712.1ms\n",
      "Iter 49770/50000, Train Loss: 3.7796, LR: 0.000002, GradNorm: 0.4217, Time: 1712.1ms\n",
      "Iter 49780/50000, Train Loss: 3.7315, LR: 0.000002, GradNorm: 0.4206, Time: 1711.0ms\n",
      "Iter 49790/50000, Train Loss: 3.7360, LR: 0.000002, GradNorm: 0.4131, Time: 1711.7ms\n",
      "Iter 49800/50000, Train Loss: 3.7393, LR: 0.000002, GradNorm: 0.4162, Time: 1712.8ms\n",
      "Iter 49810/50000, Train Loss: 3.6703, LR: 0.000002, GradNorm: 0.4144, Time: 1711.2ms\n",
      "Iter 49820/50000, Train Loss: 3.7889, LR: 0.000002, GradNorm: 0.4247, Time: 1711.1ms\n",
      "Iter 49830/50000, Train Loss: 3.7241, LR: 0.000002, GradNorm: 0.4209, Time: 1712.5ms\n",
      "Iter 49840/50000, Train Loss: 3.7485, LR: 0.000002, GradNorm: 0.4167, Time: 1712.5ms\n",
      "Iter 49850/50000, Train Loss: 3.6982, LR: 0.000002, GradNorm: 0.4155, Time: 1711.1ms\n",
      "Iter 49860/50000, Train Loss: 3.7219, LR: 0.000002, GradNorm: 0.4135, Time: 1711.5ms\n",
      "Iter 49870/50000, Train Loss: 3.7635, LR: 0.000002, GradNorm: 0.4190, Time: 1710.6ms\n",
      "Iter 49880/50000, Train Loss: 3.7722, LR: 0.000002, GradNorm: 0.4145, Time: 1711.6ms\n",
      "Iter 49890/50000, Train Loss: 3.7417, LR: 0.000002, GradNorm: 0.4120, Time: 1712.3ms\n",
      "Iter 49900/50000, Train Loss: 3.7698, LR: 0.000002, GradNorm: 0.4207, Time: 1712.5ms\n",
      "Iter 49910/50000, Train Loss: 3.7078, LR: 0.000002, GradNorm: 0.4096, Time: 1711.9ms\n",
      "Iter 49920/50000, Train Loss: 3.7709, LR: 0.000002, GradNorm: 0.4123, Time: 1711.4ms\n",
      "Iter 49930/50000, Train Loss: 3.7297, LR: 0.000002, GradNorm: 0.4162, Time: 1711.5ms\n",
      "Iter 49940/50000, Train Loss: 3.6862, LR: 0.000002, GradNorm: 0.4184, Time: 1711.1ms\n",
      "Iter 49950/50000, Train Loss: 3.6843, LR: 0.000002, GradNorm: 0.4138, Time: 1710.4ms\n",
      "Iter 49960/50000, Train Loss: 3.7084, LR: 0.000002, GradNorm: 0.4221, Time: 1711.3ms\n",
      "Iter 49970/50000, Train Loss: 3.7475, LR: 0.000002, GradNorm: 0.4133, Time: 1712.9ms\n",
      "Iter 49980/50000, Train Loss: 3.7236, LR: 0.000002, GradNorm: 0.4184, Time: 1712.3ms\n",
      "Iter 49990/50000, Train Loss: 3.7835, LR: 0.000002, GradNorm: 0.4098, Time: 1711.2ms\n",
      "Training complete! Logs written to runs/llm_run\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "writer = SummaryWriter(log_dir=\"runs/llm_run\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "iterations = []\n",
    "eval_iterations = []\n",
    "\n",
    "print(\"--- Starting Training Loop ---\")\n",
    "t0 = time.time()\n",
    "\n",
    "for iter_num in range(start_iter, training_args['max_iters']):\n",
    "\n",
    "    lr = compute_lr(\n",
    "        iter_num,\n",
    "        training_args['learning_rate'],\n",
    "        training_args['min_lr'],\n",
    "        training_args['warmup_steps'],\n",
    "        training_args['lr_decay_steps']\n",
    "    )\n",
    "    for pg in optimizer.param_groups:\n",
    "        pg['lr'] = lr\n",
    "\n",
    "\n",
    "    inputs, targets = get_batch(\n",
    "        train_data,\n",
    "        training_args['batch_size'],\n",
    "        model_args['context_length'],\n",
    "        device\n",
    "    )\n",
    "    logits = model(inputs)\n",
    "    loss = loss_init(\n",
    "        logits.view(-1, model_args['vocab_size']),\n",
    "        targets.view(-1)\n",
    "    )\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    total_norm = torch.nn.utils.clip_grad_norm_(\n",
    "        model.parameters(),\n",
    "        training_args['gradient_clip_val']\n",
    "    )\n",
    "    optimizer.step()\n",
    "\n",
    "    train_losses.append(loss.item())\n",
    "    iterations.append(iter_num)\n",
    "\n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), iter_num)\n",
    "    writer.add_scalar(\"LR\", lr, iter_num)\n",
    "    writer.add_scalar(\"GradNorm/total\", total_norm, iter_num)\n",
    "\n",
    "    if iter_num % 10 == 0:\n",
    "        t1 = time.time()\n",
    "        print(\n",
    "            f\"Iter {iter_num}/{training_args['max_iters']}, \"\n",
    "            f\"Train Loss: {loss.item():.4f}, \"\n",
    "            f\"LR: {lr:.6f}, \"\n",
    "            f\"GradNorm: {total_norm:.4f}, \"\n",
    "            f\"Time: {(t1-t0)*1000:.1f}ms\"\n",
    "        )\n",
    "        t0 = t1\n",
    "\n",
    "    if iter_num > 0 and iter_num % training_args['eval_interval'] == 0:\n",
    "        val_loss = evaluate()\n",
    "        val_losses.append(val_loss)\n",
    "        eval_iterations.append(iter_num)\n",
    "\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, iter_num)\n",
    "\n",
    "        print(f\"--- Eval at iter {iter_num}: Val Loss: {val_loss:.4f} ---\")\n",
    "        checkpoint_path = os.path.join(\n",
    "            data_args['checkpoint_dir'],\n",
    "            f\"model_iter_{iter_num}.pt\"\n",
    "        )\n",
    "        save(model, optimizer, iter_num, checkpoint_path)\n",
    "\n",
    "writer.close()\n",
    "print(\"Training complete! Logs written to runs/llm_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Training Finished! ---\n",
      "save checkpoints/model_final.pt iterations: 50000\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Training Finished! ---\")\n",
    "final_checkpoint_path = os.path.join(data_args['checkpoint_dir'], \"model_final.pt\")\n",
    "save(model, optimizer, training_args['max_iters'], final_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
